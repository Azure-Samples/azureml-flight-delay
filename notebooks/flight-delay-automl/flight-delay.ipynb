{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SDK Demo - Flight Delay\n",
        "This demo notebook shows using the AzureML Python SDK to train a classification model using AutoML like we did from the GUI.\n",
        "\n",
        "Note: the last section shows AutoML with many models, parallel runstep and pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update Environment\n",
        "\n",
        "Before running the notebook, make sure the correct versions of these libraries are installed.\n",
        "\n",
        "TODO: Confirm which packages and versions need to be installed with AzureML Python 3.8 kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1603828996898
        }
      },
      "outputs": [],
      "source": [
        "! pip install azureml-responsibleai azureml-train-automl-client azureml-widgets azureml-train-automl azureml-automl-core azureml-train-automl-runtime==1.38.0\n",
        "!pip install --upgrade azureml-responsibleai\n",
        "!pip install liac-arff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626960616085
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup working directory\n",
        "\n",
        "The cell below creates our working directory. This will hold our generated scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1626988780790
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "project_folder = './scripts'\n",
        "\n",
        "# Working directory\n",
        "if not os.path.exists(project_folder):\n",
        "    os.makedirs(project_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write helper file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/helper.py\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import json\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.run import Run\n",
        "from azureml.core import Workspace\n",
        "from azureml.train.automl._azureautomlsettings import AzureAutoMLSettings\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "\n",
        "\n",
        "def split_data(data_path):\n",
        "\n",
        "    train_data_path = os.path.join(data_path, \"upload_train_data\")\n",
        "    inference_data_path = os.path.join(data_path, \"upload_inference_data\")\n",
        "    os.makedirs(train_data_path, exist_ok=True)\n",
        "    os.makedirs(inference_data_path, exist_ok=True)\n",
        "\n",
        "    files_list = [os.path.join(path, f) for path, _, files in os.walk(data_path) for f in files\n",
        "                  if path not in (train_data_path, inference_data_path)]\n",
        "\n",
        "    for file in files_list:\n",
        "        file_name = os.path.basename(file)\n",
        "        file_extension = os.path.splitext(file_name)[1].lower()\n",
        "        df = pd.read_csv(file)\n",
        "        train_df, inference_df = df, df\n",
        "        train_df.to_csv(os.path.join(train_data_path, file_name), index=None, header=True)\n",
        "        inference_df.to_csv(os.path.join(inference_data_path, file_name), index=None, header=True)\n",
        "\n",
        "    return train_data_path, inference_data_path\n",
        "\n",
        "def validate_parallel_run_config(parallel_run_config):\n",
        "    max_concurrency = 20\n",
        "    if (parallel_run_config.process_count_per_node * parallel_run_config.node_count) > max_concurrency:\n",
        "        print(\"Please decrease concurrency to maximum of 20 as currently AutoML does not support it.\")\n",
        "        raise ValueError(\"node_count*process_count_per_node must be between 1 and max_concurrency {}\"\n",
        "                         .format(max_concurrency))\n",
        "\n",
        "\n",
        "def get_automl_environmentx(workspace: Workspace, automl_settings_dict: AzureAutoMLSettings):\n",
        "    from azureml.core import RunConfiguration\n",
        "    from azureml.train.automl._environment_utilities import modify_run_configuration\n",
        "    import logging\n",
        "    null_logger = logging.getLogger(\"manymodels_null_logger\")\n",
        "    null_logger.addHandler(logging.NullHandler())\n",
        "    null_logger.propagate = False\n",
        "    automl_settings_obj = AzureAutoMLSettings.from_string_or_dict(\n",
        "        automl_settings_dict)\n",
        "    run_configuration = modify_run_configuration(\n",
        "        automl_settings_obj,\n",
        "        RunConfiguration(),\n",
        "        logger=null_logger)\n",
        "    train_env = run_configuration.environment\n",
        "    train_env.environment_variables['DISABLE_ENV_MISMATCH'] = True\n",
        "    train_env.environment_variables['AZUREML_FLUSH_INGEST_WAIT'] = ''\n",
        "    train_env.environment_variables['AZUREML_METRICS_POLLING_INTERVAL'] = '30'\n",
        "    return run_configuration.environment\n",
        "\n",
        "\n",
        "def get_output(run, results_name, output_name):\n",
        "    # remove previous run results, if present\n",
        "    shutil.rmtree(results_name, ignore_errors=True)\n",
        "\n",
        "    parallel_run_output_file_name = \"parallel_run_step.txt\"\n",
        "\n",
        "    # download the contents of the output folder\n",
        "    batch_run = next(run.get_children())\n",
        "    batch_output = batch_run.get_output_data(output_name)\n",
        "    batch_output.download(local_path=results_name)\n",
        "\n",
        "    keep_root_folder(results_name, results_name)\n",
        "    for root, dirs, files in os.walk(results_name):\n",
        "        for file in files:\n",
        "            if file.endswith(parallel_run_output_file_name):\n",
        "                result_file = os.path.join(root, file)\n",
        "                break\n",
        "\n",
        "    return result_file\n",
        "\n",
        "\n",
        "def keep_root_folder(root_path, cur_path):\n",
        "    for filename in os.listdir(cur_path):\n",
        "        if os.path.isfile(os.path.join(cur_path, filename)):\n",
        "            shutil.move(os.path.join(cur_path, filename),\n",
        "                        os.path.join(root_path, filename))\n",
        "        elif os.path.isdir(os.path.join(cur_path, filename)):\n",
        "            keep_root_folder(root_path, os.path.join(cur_path, filename))\n",
        "        else:\n",
        "            sys.exit(\"No files found.\")\n",
        "\n",
        "    # remove empty folders\n",
        "    if root_path != cur_path:\n",
        "        os.rmdir(cur_path)\n",
        "    return\n",
        "\n",
        "def write_automl_settings_to_file(automl_settings):\n",
        "    with open('scripts//automlconfig.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(automl_settings, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def cancel_runs_in_experiment(ws, experiment):\n",
        "    failed_experiment = Experiment(ws, experiment)\n",
        "    all_runs = failed_experiment.get_runs()\n",
        "    for idx, run in enumerate(all_runs):\n",
        "        try:\n",
        "            if run.status == 'Running':\n",
        "                run = Run(failed_experiment, run.id)\n",
        "                print('Canceling run: ', run)\n",
        "                run.cancel()\n",
        "        except Exception as e:\n",
        "            print('Canceling run failed due to ', e)\n",
        "\n",
        "\n",
        "def build_parallel_run_config(train_env, compute, nodecount, workercount, timeout):\n",
        "    from azureml.pipeline.steps import ParallelRunConfig\n",
        "    parallel_run_config = ParallelRunConfig(\n",
        "        source_directory='./scripts',\n",
        "        entry_script='train_minibatch.py',\n",
        "        mini_batch_size=\"1\",  # do not modify this setting\n",
        "        run_invocation_timeout=timeout,\n",
        "        error_threshold=-1,\n",
        "        output_action=\"append_row\",\n",
        "        environment=train_env,\n",
        "        process_count_per_node=workercount,\n",
        "        compute_target=compute,\n",
        "        node_count=nodecount)\n",
        "    validate_parallel_run_config(parallel_run_config)\n",
        "    return parallel_run_config\n",
        "\n",
        "\n",
        "def get_automl_environment(workspace: Workspace, automl_settings_dict: dict):\n",
        "    return get_automl_environmentx(workspace, automl_settings_dict)\n",
        "\n",
        "\n",
        "def get_training_output(run, training_results_name, training_output_name):\n",
        "    return get_output(run, training_results_name, training_output_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define the training scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/train_helper.py\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "class MetadataFileHandler:\n",
        "\n",
        "    # Metadata file names\n",
        "    ARGS_FILE_NAME = \"args.pkl\"\n",
        "    AUTOML_SETTINGS_FILE_NAME = \"automl_settings.pkl\"\n",
        "    LOGS_FILE_NAME = \"logs.pkl\"\n",
        "    RUN_DTO_FILE_NAME = \"run_dto.pkl\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        # Directory where metadata files live\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Full paths to metadata files\n",
        "        self._args_file_path = os.path.join(self.data_dir, self.ARGS_FILE_NAME)\n",
        "        self._automl_settings_file_path = os.path.join(self.data_dir, self.AUTOML_SETTINGS_FILE_NAME)\n",
        "        self._logs_file_path = os.path.join(self.data_dir, self.LOGS_FILE_NAME)\n",
        "        self._run_dto_file_name = os.path.join(self.data_dir, self.RUN_DTO_FILE_NAME)\n",
        "\n",
        "    def delete_logs_file_if_exists(self):\n",
        "        if not os.path.exists(self._logs_file_path):\n",
        "            return\n",
        "        os.remove(self._logs_file_path)\n",
        "\n",
        "    def load_automl_settings(self):\n",
        "        return self.load_obj_from_disk(self._automl_settings_file_path)\n",
        "\n",
        "    def load_args(self):\n",
        "        return self.load_obj_from_disk(self._args_file_path)\n",
        "\n",
        "    def load_logs(self):\n",
        "        return self.load_obj_from_disk(self._logs_file_path)\n",
        "\n",
        "    def load_run_dto(self):\n",
        "        return self.load_obj_from_disk(self._run_dto_file_name)\n",
        "\n",
        "    def write_args_to_disk(self, args):\n",
        "        self.serialize_obj_to_disk(args, self._args_file_path)\n",
        "\n",
        "    def write_automl_settings_to_disk(self, automl_settings):\n",
        "        self.serialize_obj_to_disk(automl_settings, self._automl_settings_file_path)\n",
        "\n",
        "    def write_logs_to_disk(self, logs):\n",
        "        self.serialize_obj_to_disk(logs, self._logs_file_path)\n",
        "\n",
        "    def write_run_dto_to_disk(self, run_dto):\n",
        "        self.serialize_obj_to_disk(run_dto, self._run_dto_file_name)\n",
        "\n",
        "    @classmethod\n",
        "    def load_obj_from_disk(cls, file_path):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    @classmethod\n",
        "    def serialize_obj_to_disk(cls, obj, file_path):\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "class TrainUtil:\n",
        "\n",
        "    @staticmethod\n",
        "    def str2bool(v):\n",
        "\n",
        "        if isinstance(v, bool):\n",
        "            return v\n",
        "        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "            return True\n",
        "        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "            return False\n",
        "        else:\n",
        "            raise argparse.ArgumentTypeError('Boolean value expected.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/train_minibatch.py\n",
        "\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "from multiprocessing import current_process\n",
        "from pathlib import Path\n",
        "from random import randint\n",
        "from subprocess import PIPE, Popen\n",
        "from time import sleep\n",
        "\n",
        "import pandas as pd\n",
        "from azureml.automl.core.shared import log_server\n",
        "from azureml.core import Run\n",
        "\n",
        "from train_helper import MetadataFileHandler, TrainUtil\n",
        "\n",
        "\n",
        "# This is used by UI to display the many model settings\n",
        "many_model_run_properties = {'many_models_run': True}\n",
        "\n",
        "parser = argparse.ArgumentParser(\"split\")\n",
        "parser.add_argument(\"--process_count_per_node\", default=1, type=int, help=\"number of processes per node\")\n",
        "parser.add_argument(\n",
        "    \"--retrain_failed_models\", default=False, type=TrainUtil.str2bool, help=\"retrain failed models only\")\n",
        "\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "\n",
        "def read_from_json():\n",
        "    full_path = Path(__file__).absolute().parent\n",
        "    with open(str(full_path) + \"/automlconfig.json\") as json_file:\n",
        "        return json.load(json_file)\n",
        "\n",
        "\n",
        "automl_settings = read_from_json()\n",
        "current_step_run = Run.get_context()\n",
        "metadata_file_handler = MetadataFileHandler(tempfile.mkdtemp())\n",
        "\n",
        "target_column = automl_settings.get('label_column_name', None)\n",
        "\n",
        "print(\"target_column: {}\".format(target_column))\n",
        "print(\"retrain_failed_models: {}\".format(args.retrain_failed_models))\n",
        "\n",
        "\n",
        "def init():\n",
        "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
        "    working_dir = os.environ.get(\"AZ_BATCHAI_OUTPUT_logs\", \"\")\n",
        "    ip_addr = os.environ.get(\"AZ_BATCHAI_WORKER_IP\", \"\")\n",
        "    log_dir = os.path.join(working_dir, \"user\", ip_addr, current_process().name)\n",
        "    t_log_dir = Path(log_dir)\n",
        "    t_log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    automl_settings['many_models'] = True\n",
        "    automl_settings['many_models_process_count_per_node'] = args.process_count_per_node\n",
        "\n",
        "    # Try stopping logging server in the parent minibatch process.\n",
        "    # Otherwise, the logging server will progressively consume more and more CPU, leading to\n",
        "    # CPU starvation on the box. TODO: diagnose why this happens and fix\n",
        "    try:\n",
        "        log_server.server.stop()\n",
        "    except Exception as e:\n",
        "        print(\"Stopping the AutoML logging server in the entry script parent process failed with exception: {}\"\n",
        "              .format(e))\n",
        "\n",
        "    debug_log = automl_settings.get('debug_log', None)\n",
        "    if debug_log is not None:\n",
        "        automl_settings['debug_log'] = os.path.join(log_dir, debug_log)\n",
        "        automl_settings['path'] = tempfile.mkdtemp()\n",
        "        print(f\"{__file__}.AutoML debug log:{automl_settings['debug_log']}\")\n",
        "\n",
        "    # Write metadata files to disk, so they can be consumed by subprocesses that run AutoML\n",
        "    metadata_file_handler.write_args_to_disk(args)\n",
        "    metadata_file_handler.write_automl_settings_to_disk(automl_settings)\n",
        "    metadata_file_handler.write_run_dto_to_disk(current_step_run._client.run_dto)\n",
        "\n",
        "    print(f\"{__file__}.output_folder:{output_folder}\")\n",
        "    print(\"init()\")\n",
        "    sleep(randint(1, 120))\n",
        "\n",
        "\n",
        "def run(input_data_files):\n",
        "    print(\"Entering run()\")\n",
        "    os.makedirs('./outputs', exist_ok=True)\n",
        "    resultList = []\n",
        "    for input_data_file in input_data_files:\n",
        "        print(\"Launch subprocess to run AutoML on the data\")\n",
        "        env = os.environ.copy()\n",
        "        # Aggressively buffer I/O from the subprocess\n",
        "        env['PYTHONUNBUFFERED'] = '0'\n",
        "        subprocess = Popen([\n",
        "            sys.executable,\n",
        "            os.path.join(os.path.dirname(os.path.realpath(__file__)), 'train_model.py'),\n",
        "            input_data_file,\n",
        "            metadata_file_handler.data_dir], env=env, stdout=PIPE, stderr=PIPE)\n",
        "        for line in subprocess.stdout:\n",
        "            print(line.decode().rstrip())\n",
        "        subprocess.wait()\n",
        "        print(\"Subprocess completed with exit code: {}\".format(subprocess.returncode))\n",
        "        subprocess_stderr = subprocess.stderr.read().decode().rstrip()\n",
        "        if subprocess_stderr:\n",
        "            print(\"stderr from subprocess:\\n{}\\n\".format(subprocess_stderr))\n",
        "        if subprocess.returncode != 0:\n",
        "            raise Exception(\"AutoML training subprocess exited unsuccesffuly with error code: {}\\n\"\n",
        "                            \"stderr from subprocess: \\n{}\\n\".format(subprocess.returncode, subprocess_stderr))\n",
        "        logs = metadata_file_handler.load_logs()\n",
        "        resultList.append(logs)\n",
        "        metadata_file_handler.delete_logs_file_if_exists()\n",
        "    print(\"Constructing DataFrame from results\")\n",
        "    result = pd.DataFrame(data=resultList)\n",
        "    print(\"Ending run()\\n\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $project_folder/train_model.py\n",
        "\n",
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "from train_helper import MetadataFileHandler\n",
        "\n",
        "# TODO: Remove once Batch AI has fixed this issue.\n",
        "# Exclude mounted blobfuse folders from sys.path, preventing Python from scanning\n",
        "# folders in the blob container when resolving import statements. This significantly reduces traffic\n",
        "# to the storage account.\n",
        "import sys\n",
        "sys.path = [p for p in sys.path if not p.startswith('/mnt/batch')]\n",
        "\n",
        "import datetime  # noqa: E402\n",
        "import hashlib  # noqa: E402\n",
        "import os  # noqa: E402\n",
        "\n",
        "import pandas as pd  # noqa: E402\n",
        "from azureml.automl.core.shared import constants  # noqa: E402\n",
        "from azureml.automl.core.shared.exceptions import AutoMLException, ClientException, ErrorTypes  # noqa: E402\n",
        "from azureml.automl.core.shared.utilities import get_error_code  # noqa: E402\n",
        "from azureml.core import Run  # noqa: E402\n",
        "from azureml.core.model import Model  # noqa: E402\n",
        "from azureml.train.automl import AutoMLConfig  # noqa: E402\n",
        "\n",
        "\n",
        "# This is used by UI to display the many model settings\n",
        "many_model_run_properties = {'many_models_run': True}\n",
        "\n",
        "\n",
        "def compose_logs(file_name, model, start_time):\n",
        "    logs = []\n",
        "    logs.append('AutoML')\n",
        "    logs.append(file_name)\n",
        "    logs.append(None)\n",
        "    logs.append(None)\n",
        "    logs.append(model.name)\n",
        "    logs.append(model.tags)\n",
        "    logs.append(start_time)\n",
        "    logs.append(datetime.datetime.now())\n",
        "    logs.append(None)\n",
        "    logs.append(None)\n",
        "    logs.append(None)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(file_path, data, automl_settings, current_step_run):\n",
        "    file_name = file_path.split('/')[-1][:-4]\n",
        "    print(file_name)\n",
        "    print(\"in train_model\")\n",
        "    print('data')\n",
        "    print(data.head(5))\n",
        "    print(automl_settings)\n",
        "    automl_config = AutoMLConfig(training_data=data, **automl_settings)\n",
        "\n",
        "    print(\"submit_child\")\n",
        "    local_run = current_step_run.submit_child(automl_config, show_output=True)\n",
        "\n",
        "    local_run.add_properties({\n",
        "        k: str(many_model_run_properties[k])\n",
        "        for k in many_model_run_properties\n",
        "    })\n",
        "\n",
        "    print(local_run)\n",
        "\n",
        "    best_child_run, fitted_model = local_run.get_output()\n",
        "\n",
        "    return fitted_model, local_run, best_child_run\n",
        "\n",
        "\n",
        "def run(file_path, args, automl_settings, current_step_run):\n",
        "    model_name = None\n",
        "    current_run = None\n",
        "    error_message = None\n",
        "    error_code = None\n",
        "    error_type = None\n",
        "    tags_dict = None\n",
        "\n",
        "    logs = []\n",
        "    date1 = datetime.datetime.now()\n",
        "    print('start (' + file_path + ') ' + str(datetime.datetime.now()))\n",
        "\n",
        "    file_name_with_extension = os.path.basename(file_path)\n",
        "    file_name, file_extension = os.path.splitext(file_name_with_extension)\n",
        "\n",
        "    try:\n",
        "        if file_extension.lower() == \".parquet\":\n",
        "            data = pd.read_parquet(file_path)\n",
        "        else:\n",
        "            data = pd.read_csv(file_path)\n",
        "\n",
        "        tags_dict = {'ModelType': 'AutoML'}\n",
        "\n",
        "        if args.retrain_failed_models:\n",
        "            print('querying for existing models')\n",
        "            try:\n",
        "                tags = [[k, v] for k, v in tags_dict.items()]\n",
        "                models = Model.list(current_step_run.experiment.workspace, tags=tags, latest=True)\n",
        "\n",
        "                if models:\n",
        "                    print(\"model already exists for the dataset \" + models[0].name)\n",
        "                    return compose_logs(file_name, models[0], date1)\n",
        "            except Exception as error:\n",
        "                print('Failed to list the models. ' + 'Error message: ' + str(error))\n",
        "\n",
        "        tags_dict.update({'InputData': file_name_with_extension})\n",
        "        tags_dict.update({'StepRunId': current_step_run.id})\n",
        "        tags_dict.update({'RunId': current_step_run.parent.id})\n",
        "\n",
        "        # train model\n",
        "        many_model_run_properties['many_models_input_file'] = file_name_with_extension\n",
        "\n",
        "        fitted_model, current_run, best_child_run = train_model(file_path, data, automl_settings, current_step_run)\n",
        "        model_string = '_'.join(file_name)\n",
        "        print(\"model string to encode \" + model_string)\n",
        "        sha = hashlib.sha256()\n",
        "        sha.update(model_string.encode())\n",
        "        model_name = 'automl_' + sha.hexdigest()\n",
        "        tags_dict.update({'Hash': sha.hexdigest()})\n",
        "        try:\n",
        "            print('done training')\n",
        "            print('Trained best model ' + model_name)\n",
        "\n",
        "            print(best_child_run)\n",
        "            print(fitted_model)\n",
        "            print(model_name)\n",
        "\n",
        "            print('register model')\n",
        "\n",
        "            best_child_run.register_model(\n",
        "                model_name=model_name, model_path=constants.MODEL_PATH, description='AutoML', tags=tags_dict)\n",
        "            print('Registered ' + model_name)\n",
        "        except Exception as error:\n",
        "            error_type = ErrorTypes.Unclassified\n",
        "            error_message = 'Failed to register the model. ' + 'Error message: ' + str(error)\n",
        "            print(error_message)\n",
        "\n",
        "        date2 = datetime.datetime.now()\n",
        "\n",
        "        logs.append('AutoML')\n",
        "        logs.append(file_name)\n",
        "        logs.append(current_run.id)\n",
        "        logs.append(current_run.get_status())\n",
        "        logs.append(model_name)\n",
        "        logs.append(tags_dict)\n",
        "        logs.append(str(date1))\n",
        "        logs.append(str(date2))\n",
        "        logs.append(error_type)\n",
        "        logs.append(error_code)\n",
        "        logs.append(error_message)\n",
        "\n",
        "        print('ending (' + file_path + ') ' + str(date2))\n",
        "\n",
        "    # 10.1 Log the error message if an exception occurs\n",
        "    except (ClientException, AutoMLException) as error:\n",
        "        date2 = datetime.datetime.now()\n",
        "        error_message = 'Failed to train the model. ' + 'Error : ' + str(error)\n",
        "\n",
        "        logs.append('AutoML')\n",
        "        logs.append(file_name)\n",
        "\n",
        "        if current_run:\n",
        "            logs.append(current_run.id)\n",
        "            logs.append(current_run.get_status())\n",
        "        else:\n",
        "            logs.append(current_run)\n",
        "            logs.append('Failed')\n",
        "\n",
        "        logs.append(model_name)\n",
        "        logs.append(tags_dict)\n",
        "        logs.append(str(date1))\n",
        "        logs.append(str(date2))\n",
        "        if isinstance(error, AutoMLException):\n",
        "            logs.append(error.error_type)\n",
        "        else:\n",
        "            logs.append(None)\n",
        "        logs.append(get_error_code(error))\n",
        "        logs.append(error_message)\n",
        "\n",
        "        print(error_message)\n",
        "        print('ending (' + file_path + ') ' + str(date2))\n",
        "\n",
        "    return logs\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_file_path = sys.argv[1]\n",
        "    data_dir = sys.argv[2]\n",
        "    metadata_file_handler = MetadataFileHandler(data_dir)\n",
        "    args = metadata_file_handler.load_args()\n",
        "    automl_settings = metadata_file_handler.load_automl_settings()\n",
        "    run_dto = metadata_file_handler.load_run_dto()\n",
        "    experiment, run_id = Run._load_scope()\n",
        "    current_step_run = Run(experiment, run_id, _run_dto=run_dto)\n",
        "    logs = run(data_file_path, args, automl_settings, current_step_run)\n",
        "    metadata_file_handler.write_logs_to_disk(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import and verify the Azure ML SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626988795206
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "\n",
        "azureml.core.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data from Azure Dataset Registry\n",
        "\n",
        "First step is to get our data using the Dataset module, the function `Dataset.get_by_name()` returns a registered Dataset from a given `workspace` and its registration `name`.\n",
        "\n",
        "Then the tabular dataset ios converted to a Pandas Dataframe \n",
        "\n",
        "For more information on **Dataset**, please visit: [Microsoft Dataset Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626988834956
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core import Dataset\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "tabular = Dataset.get_by_name(ws, '/flightdelay/flightdelayweather_ds')\n",
        "\n",
        "data = tabular.to_pandas_dataframe()\n",
        "tabular.take(3).to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create AzureML Compute Cluster\n",
        "\n",
        "Firstly, check for the existence of the cluster. If it already exists, we are able to reuse it. Checking for the existence of the cluster can be performed by calling the constructor `ComputeTarget()` with the current workspace and name of the cluster.\n",
        "\n",
        "In case the cluster does not exist, the next step will be to provide a configuration for the new AML cluster by calling the function `AmlCompute.provisioning_configuration()`. It takes as parameters the VM size and the max number of nodes that the cluster can scale up to. After the configuration has executed, `ComputeTarget.create()` should be called with the previously configuration object and the workspace object.\n",
        "\n",
        "For more information on **ComputeTarget**, please visit: [Microsoft ComputeTarget Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py)\n",
        "\n",
        "For more information on **AmlCompute**, please visit: [Microsoft AmlCompute Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.akscompute?view=azure-ml-py)\n",
        "\n",
        "\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626988835502
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "### Create AML CPU Compute Cluster\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name='cpu-cluster')\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new compute target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2',\n",
        "                                                           max_nodes=4)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, 'cpu-cluster', compute_config)\n",
        "\n",
        "    compute_target.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Dataset\n",
        "This step uploads the training data and creates a dataset if it was not previously created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "datastore = Datastore.get(ws, 'workspaceblobstore')\n",
        "ds = Dataset.File.upload_directory(src_dir='../../azureStorageFiles',\n",
        "    target=DataPath(datastore,'/flightdelay'),\n",
        "    overwrite=True,\n",
        "    show_progress=True)\n",
        "\n",
        "flight_dataset_2008_with_weather = Dataset.Tabular.from_delimited_files(path=[(datastore, '/flightdelay/flight_dataset_2008_with_weather.csv')])\n",
        "\n",
        "flightdelayweather_ds = flight_dataset_2008_with_weather.register(workspace=ws, name='/flightdelay/flightdelayweather_ds', create_new_version=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![automl](./automl.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate an Automated ML Config\n",
        "\n",
        "Before the execution of an Automated ML run, the `AutoMLConfig` should be setup. `AutoMLConfig` is a configuration object that contains and persists the parameters for configuring the experiment run parameters. This configuration is a key element in the execution of the run since it defines things such as the number of iterations and primary metric to optimize on. In the example below the run will be setup to execute a regression task with 25 iterations and using `accuracy` as primary metric.\n",
        "\n",
        "For more information on **AutoMLConfig**, please visit: [Microsoft AutoMLConfig Documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig?view=azure-ml-py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1626988836123
        }
      },
      "outputs": [],
      "source": [
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "training_data, validation_data = tabular.random_split(percentage=0.9, seed=1)\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             max_iterations = 3,\n",
        "                             iteration_timeout_minutes = 5, \n",
        "                             max_cores_per_iteration = 4,\n",
        "                             primary_metric = 'accuracy',\n",
        "                             debug_log = 'automl.log',\n",
        "                             training_data = training_data,\n",
        "                             validation_data = validation_data,\n",
        "                             label_column_name = \"ArrDelay15\",\n",
        "                             compute_target = compute_target,\n",
        "                             path = project_folder,\n",
        "                             model_explainability = True,\n",
        "                             experiment_exit_score = 0.9,\n",
        "                             enable_early_stopping = True,\n",
        "                             enable_onnx_compatible_models=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run our Experiment on the Compute Cluster\n",
        "\n",
        "The Experiment constructor allows to create an experiment instance. The constructor takes in the current workspace, which is fetched by calling `Workspace.from_config()` and an experiment name. \n",
        "\n",
        "The `experiment.submit()` function is called to send the experiment for execution. The only parameter received by this function is the `AutoMLConfig` object instantiated previously in this module.\n",
        "\n",
        "For more information on **Experiment**, please visit: [Microsoft Experiment Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py)\n",
        "\n",
        "The Compute Cluster will take a few minutes to scale up. You can monitor this in the Studio from the link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626988846213
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "# Get an instance of the Workspace from the config file\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Create Experiment\n",
        "experiment = Experiment(ws, 'flight-delay-exp')\n",
        "\n",
        "remote_run = experiment.submit(automl_config, show_output=False)\n",
        "remote_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Automated ML Run Details\n",
        "\n",
        "The creation of an object of type `AutoMLRun` will enable us to observe the experiment progress and results. The object is created by calling the constructor `AutoMLRun()`. It takes as arguments the experiment and the identifier of the run to fetch. After the object has been instantiated, the `RunDetails()` function will retrieve the progress, metrics, and tasks for the specified run. They will be displayed by calling the function `show()` over the mentioned object.\n",
        "\n",
        "For more information on **AutoMLRun**, please visit: [Microsoft AutoMLRun Documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.run.automlrun?view=azure-ml-py)\n",
        "\n",
        "For more information on **RunDetails**, please visit: [Microsoft RunDetails Documentation](https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets.rundetails?view=azure-ml-py)\n",
        "\n",
        "\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward. (Status should be **Completed**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626988850024
        }
      },
      "outputs": [],
      "source": [
        "from azureml.train.automl.run import AutoMLRun \n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "experiment = Experiment(ws, 'flight-delay-exp')\n",
        "remote_run = AutoMLRun(experiment=experiment, run_id=remote_run.id)\n",
        "\n",
        "RunDetails(remote_run).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show best run\n",
        "\n",
        "Select the best model from your iterations. The `get_output` function returns the best run and the fitted model for the last fit invocation. By using the overloads on get_output, you can retrieve the best run and fitted model for any logged metric or a particular iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989433787
        }
      },
      "outputs": [],
      "source": [
        "remote_run.wait_for_completion()\n",
        "best_run, fitted_model = remote_run.get_output()\n",
        "print(best_run)\n",
        "print(fitted_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show best model type\n",
        "\n",
        "Models generated by AutoML are not black boxes and can be inspected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989434031
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class balancing detection\n",
        "\n",
        "Imbalanced data is commonly found in data for machine learning classification scenarios, and refers to data that contains a disproportionate ratio of observations in each class. This imbalance can lead to a falsely perceived positive effect of a model's accuracy, because the input data has bias towards one class, which results in the trained model to mimic that bias.\n",
        "\n",
        "`get_guardraild()` = Function which prints and returns detailed results from running Guardrail verification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989434255
        }
      },
      "outputs": [],
      "source": [
        "guardrails = remote_run.get_guardrails()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "In Azure Machine Learning, data-scaling and normalization techniques are applied to make feature engineering easier. Collectively, these techniques and this feature engineering are called featurization in automated machine learning, or AutoML, experiments.\n",
        "\n",
        "Automated ML makes it transparent to get this information from the fitted_model output from automated ML.\n",
        "\n",
        "By calling `get_featurization_summary()` we will be able to retrieve the featurization summary for all the input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989434443
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "feature_engineered_sum = pd.DataFrame(fitted_model.named_steps['datatransformer'].get_featurization_summary())\n",
        "feature_engineered_sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Engineered Features Importance\n",
        "\n",
        "The ExplanationClient object defines the client that uploads and downloads explanations. The best run is passed as an argument, and from it the explanations are stored as raw_explanations.\n",
        "\n",
        "For more information on **ExplanationClient**, please visit: [Microsoft ExplanationClient Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.explanationclient?view=azure-ml-py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989434777
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from azureml.interpret import ExplanationClient\n",
        "import seaborn as sns\n",
        "\n",
        "client = ExplanationClient.from_run(best_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raw Features Importance\n",
        "\n",
        "The `download_model_explanation()` function downloads a model explanation that has been stored in run history.\n",
        "\n",
        "The  `get_feature_importance_dict()` function prints out a dictionary that holds the top feature names and values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989557027
        }
      },
      "outputs": [],
      "source": [
        "raw_explanations = client.download_model_explanation(raw=True)\n",
        "summ = pd.DataFrame(raw_explanations.get_feature_importance_dict(), index = ['Importance'])\n",
        "with sns.plotting_context('notebook', font_scale=1.4):\n",
        "    plt.subplots(figsize=(13,9))\n",
        "    sns.barplot(data=summ.iloc[:10,:10], orient='h').set_title('Raw Explanations Feature Importance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989557409
        }
      },
      "outputs": [],
      "source": [
        "print(\"You can visualize the engineered explanations under the 'Explanations' tab in the AutoML run at:\\n\" + best_run.get_portal_url())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible ML\n",
        "\n",
        "## InterpretML\n",
        "\n",
        "Interpretability is critical for data scientists, auditors, and business decision makers alike to ensure compliance with company policies, industry standards, and government regulations\n",
        "\n",
        "Using the classes and methods in the SDK, you can:\n",
        "\n",
        "* Explain model prediction by generating feature importance values for the entire model and/or individual datapoints.\n",
        "* Achieve model interpretability on real-world datasets at scale, during training and inference.\n",
        "* Use an interactive visualization dashboard to discover patterns in data and explanations at training time\n",
        "\n",
        "As our next step we will retrieve our trained model and instantiate the Explainability Dashboard with the data we encoded above.\n",
        "\n",
        "After the Explainability Dashboard has loaded you will be able to navigate through the user interface to identify the most important features of your new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989590282
        }
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from interpret_community.widget import ExplanationDashboard\n",
        "from azureml.interpret import ExplanationClient\n",
        "\n",
        "best_run.download_file('outputs/model.pkl')\n",
        "model_testing = joblib.load('model.pkl')\n",
        "\n",
        "explanations = client.download_model_explanation(raw=True)\n",
        "\n",
        "val = validation_data.to_pandas_dataframe()\n",
        "val = val.drop(columns=['ArrDelay15'])\n",
        "ExplanationDashboard(explanations, model_testing, datasetX=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fairlearn\n",
        "\n",
        "Artificial intelligence and machine learning systems can display unfair behavior.\n",
        "\n",
        "Let's use Fairlearn open-source Python package with Azure Machine Learning to perform the following tasks:\n",
        "\n",
        "* Assess the fairness of your model predictions. To learn more about fairness in machine learning, see the fairness in machine learning article.\n",
        "* Upload, list and download fairness assessment insights to/from Azure Machine Learning studio.\n",
        "* See a fairness assessment dashboard in Azure Machine Learning studio to interact with your model(s)' fairness insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1626989628755
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "Y = le.fit_transform(data['ArrDelay15'].values)\n",
        "\n",
        "X_train = data.drop(columns=['ArrDelay15'])\n",
        "X_test = data.drop(columns=['ArrDelay15'])\n",
        "Y_train = Y\n",
        "A_test = data['UniqueCarrier'].to_frame()\n",
        "Y_test = Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Display Fairlearn Dashboard\n",
        "\n",
        "Let's assess how a models predictions impact different groups, and also for comparing multiple models along different fairness and performance metrics.\n",
        "\n",
        "The FairlearnDashboard class, wraps the dashboard component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989706319
        }
      },
      "outputs": [],
      "source": [
        "from fairlearn.widget import FairlearnDashboard\n",
        "\n",
        "model_2 = model_testing.fit(X_train, Y_train)\n",
        "preds = model_testing.predict(X_test)\n",
        "\n",
        "FairlearnDashboard(sensitive_features=A_test,\n",
        "                   sensitive_feature_names=['UniqueCarrier'],\n",
        "                   y_true=Y_test.tolist(),\n",
        "                   y_pred=[preds.tolist()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register Model\n",
        "\n",
        "Next, register the model obtained from the best run. In order to register the model, the function `register_model()` should be called. This will take care of registering the model obtained from the best run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989706670
        }
      },
      "outputs": [],
      "source": [
        "# register the model for deployment\n",
        "model = best_run.register_model(model_name='flight_delay_weather', \n",
        "                                model_path='outputs/model.pkl',\n",
        "                                datasets=[(Dataset.Scenario.TRAINING, tabular)],\n",
        "                                description='This model was developed by researchers at OpenAI to help us understand how the capabilities of language model capabilities scale as a function of the size of the models',\n",
        "                       tags={'title': 'Flight Delay Model',\n",
        "    'datasheet_description':\n",
        "\"\"\"\n",
        "Last updated: October 2020\n",
        "\n",
        "Based on dataset from by [Statistical Computing Statistical Graphics](http://stat-computing.org/dataexpo/2009/the-data.html)\n",
        "\n",
        "\"\"\",\n",
        "    'details': 'This model was developed for Microsoft.',\n",
        "    'date': 'October 2020, trained on data that cuts off at the end of 2008.', \n",
        "    'type': 'Classification model',\n",
        "    'version': '1.0',\n",
        "    'help': 'https://www.azure.com/',\n",
        "    'usecase_primary': \n",
        "\"\"\"\n",
        "Developed for Flight Delay Demo.\n",
        "\n",
        "\"\"\",\n",
        "    'usecase_secondary':\n",
        "\"\"\"\n",
        "Field demos and marketing.\n",
        "\n",
        "\"\"\",\n",
        "    'usecase_outofscope':\n",
        "\"\"\"\n",
        "Do not use for production environments.\n",
        "\n",
        "\"\"\",\n",
        "    'dataset_description':\n",
        "\"\"\"\n",
        "The data comes originally from RITA where it is described in detail. You can download the data there, or from the bzipped csv files listed below. These files have derivable variables removed, are packaged in yearly chunks and have been more heavily compressed than the originals.\n",
        "\n",
        "\"\"\",\n",
        "    'motivation': 'Demo the main features behind the Azure ML Workspace environment',\n",
        "    'caveats':\n",
        "\"\"\"\n",
        "\"\"\"})\n",
        "\n",
        "print(\"Model name: \" + model.name, \"Model version: \" + str(model.version), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Fairlearn Dashboard to run\n",
        "\n",
        "Create a dashboard dictionary using Fairlearn's metrics package. The `_create_group_metric_set` method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1626989760880
        }
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
        "\n",
        "#  Create a dictionary of model(s) you want to assess for fairness \n",
        "sf = { 'UniqueCarrier': A_test.UniqueCarrier }\n",
        "ys_pred = { model.id: model_testing.predict(X_test) }\n",
        "\n",
        "dash_dict = _create_group_metric_set(y_true=Y_test,\n",
        "                                    predictions=ys_pred,\n",
        "                                    sensitive_features=sf,\n",
        "                                    prediction_type='binary_classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now we can fetch the Experiment, then a Run, and upload our dashboard to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626989766821
        }
      },
      "outputs": [],
      "source": [
        "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\n",
        "\n",
        "run = experiment.start_logging(snapshot_directory=None)\n",
        "try:\n",
        "    dashboard_title = \"Fairness in Flight Delay Classifier\"\n",
        "    upload_id = upload_dashboard_dictionary(run,\n",
        "                                        dash_dict,\n",
        "                                        dashboard_name=dashboard_title)\n",
        "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
        "\n",
        "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
        "    \n",
        "    \n",
        "finally:\n",
        "    run.complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deployment\n",
        "\n",
        "## Create managed-endpoints directory\n",
        "\n",
        "Create a new directory to hold the configuration files for deploying a managed endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "managed_endpoints = './managed-endpoints'\n",
        "\n",
        "# Working directory\n",
        "if not os.path.exists(managed_endpoints):\n",
        "    os.makedirs(managed_endpoints)\n",
        "    \n",
        "if os.path.exists(os.path.join(managed_endpoints,\".amlignore\")):\n",
        "  os.remove(os.path.join(managed_endpoints,\".amlignore\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Scoring File\n",
        "\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome.\n",
        "\n",
        "The first thing to do in the scoring file is to fetch the model. This is done by calling `Model.get_model_path()` and passing the model name as a parameter.\n",
        "\n",
        "After the model has been loaded, the function `model.predict()` function should be called to start the scoring process.\n",
        "\n",
        "For more information on **Machine Learning - Score**, please visit: [Microsoft Machine Learning - Score Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $managed_endpoints/score.py\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import joblib\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azureml.automl.core\n",
        "from azureml.core.model import Model\n",
        " \n",
        "def init():\n",
        "    global model\n",
        "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model.pkl\")\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "def run(data):\n",
        "    try:\n",
        "        data = json.loads(data)[\"data\"]\n",
        "        df = pd.DataFrame(data, columns=['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp']) \n",
        "        result = model.predict(df)\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "        return {\"error\": result}\n",
        "    return {\"result\":result.tolist()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the environment definition\n",
        "\n",
        "The following file contains the details of the environment to host the model and code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $managed_endpoints/score-new.yml\n",
        "name: fd-endpoint-managed-env\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- pip:\n",
        "  - azureml-sdk[notebooks,automl]~=1.32.0\n",
        "  - azureml-defaults~=1.32.0\n",
        "  - inference-schema\n",
        "  - azureml-monitoring\n",
        "  - shap==0.39.0\n",
        "- numpy\n",
        "- scikit-learn==0.22.1\n",
        "channels:\n",
        "- anaconda\n",
        "- conda-forge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the endpoint configuration\n",
        "Specific inputs are required to deploy a model on an online endpoint:\n",
        "\n",
        "1. Model files.\n",
        "1. The code that's required to score the model.\n",
        "1. An environment in which your model runs.\n",
        "1. Settings to specify the instance type and scaling capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $managed_endpoints/endpointconfig.yml\n",
        "name: fd-endpoint-managed\n",
        "type: online\n",
        "auth_mode: key\n",
        "traffic:\n",
        "  blue: 100\n",
        "\n",
        "deployments:\n",
        "  #blue deployment\n",
        "  - name: blue\n",
        "    model: azureml:flight_delay_weather:1\n",
        "    code_configuration:\n",
        "      code:\n",
        "        local_path: ./\n",
        "      scoring_script: score.py\n",
        "    environment: \n",
        "      name: fd-endpoint-managed-env\n",
        "      version: 1\n",
        "      path: ./\n",
        "      conda_file: file:./score-new.yml\n",
        "      docker:\n",
        "          image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n",
        "    instance_type: Standard_DS3_v2\n",
        "    scale_settings:\n",
        "      scale_type: manual\n",
        "      instance_count: 1\n",
        "      min_instances: 1\n",
        "      max_instances: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy your managed online endpoint to Azure\n",
        "\n",
        "This deployment might take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment will finish processing more quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!az ml endpoint create -g [your resource group name] -w [your AML workspace name] -n fd-managed-endpoint -f ./managed-endpoints/endpointconfig.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a sample request JSON file\n",
        "\n",
        "Export some test data to a JSON file we can send to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $managed_endpoints/sample-request.json\n",
        "{\"data\": [\n",
        "[1, 30, 3, 1312, 1459, \"DL\", 287, \"CVG\", \"LAX\", 1900, 39.04614278, -84.6621725, \"KY\", 33.94253611, -118.4080744, \"CA\", 35942.4, 36979.2, 9.0, 0.0, 268.8, 304.0, 8.0, 0.0, 5.0, 15.5, -10.5, 6.0, 280.0, 920.0],\n",
        "[5, 30, 5, 1920, 2202, \"OH\", 102, \"HSV\", \"DCA\", 613, 34.6404475, -86.77310944, \"AL\", 38.85208333, -77.03772222, \"VA\", 50803.2, 52185.6, 0.0, 0.0, 393.6, 499.2, 0.0, 0.0, 28.5, 26.5, 18.0, 11.5, 2080.0, 1360.0],\n",
        "[7, 14, 1, 1310, 1550, \"UA\", 160, \"LAX\", \"SEA\", 954, 33.94253611, -118.4080744, \"CA\", 47.44898194, -122.3093131, \"WA\", 50457.6, 55296.0, 0.0, 0.0, 441.6, 454.4, 0.0, 0.0, 27.0, 26.5, 19.0, 12.5, 840.0, 1080.0],\n",
        "[3, 4, 2, 1735, 2100, \"AA\", 265, \"BOS\", \"DFW\", 1562, 42.3643475, -71.00517917, \"MA\", 32.89595056, -97.0372, \"TX\", 40089.6, 41126.4, 10.0, 7.0, 316.8, 262.4, 64.0, 0.0, 14.5, 12.5, 2.0, 0.5, 720.0, 640.0],\n",
        "[7, 18, 5, 730, 945, \"AA\", 75, \"ORD\", \"DTW\", 235, 41.979595, -87.90446417, \"IL\", 42.21205889, -83.34883583, \"MI\", 52876.8, 52876.8, 0.0, 0.0, 329.6, 396.8, 0.0, 0.0, 31.5, 32.0, 22.5, 20.0, 2720.0, 2320.0],\n",
        "[10, 15, 3, 1750, 1850, \"AA\", 60, \"DFW\", \"SAT\", 247, 32.89595056, -97.0372, \"TX\", 29.53369444, -98.46977778, \"TX\", 40089.6, 40435.2, 17.0, 9.0, 172.8, 185.6, 0.0, 0.0, 25.0, 30.0, 17.0, 20.5, 1920.0, 2400.0],\n",
        "[5, 14, 3, 1835, 1928, \"US\", 53, \"BWI\", \"PHL\", 90, 39.17540167, -76.66819833, \"MD\", 39.87195278, -75.24114083, \"PA\", 50803.2, 51148.8, 0.0, 0.0, 499.2, 505.6, 0.0, 0.0, 22.5, 24.5, 8.5, 8.5, 1120.0, 1120.0],\n",
        "[2, 27, 3, 1600, 1830, \"FL\", 150, \"LGA\", \"ATL\", 761, 40.77724306, -73.87260917, \"NY\", 33.64044444, -84.42694444, \"GA\", 39398.4, 40435.2, 0.0, 7.0, 352.0, 256.0, 0.0, 0.0, 7.0, 9.5, -4.0, -2.5, 440.0, 520.0],\n",
        "[11, 24, 1, 1837, 2015, \"OO\", 98, \"LNK\", \"ORD\", 466, 40.85097222, -96.75925, \"NE\", 41.979595, -87.90446417, \"IL\", 33868.8, 33523.2, 0.0, 2.0, 240.0, 172.8, 0.0, 0.0, 11.0, 5.0, -4.5, -2.0, 440.0, 520.0]]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Invoke the endpoint to score data by using your model\n",
        "\n",
        "You can use either the invoke command or a REST client of your choice to invoke the endpoint and score against it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!az ml endpoint invoke -g [your resource group name] -w [your AML workspace name] -n fd-managed-endpoint --request-file ./managed-endpoints/sample-request.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![pipeline](./pipeline.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution Accelerator: Many Models\n",
        "\n",
        "In the real world, many problems can be too complex to be solved by a single machine learning model. Whether that be predicting sales for each individual store, building a predictive maintanence model for hundreds of oil wells, or tailoring an experience to individual users, building a model for each instance can lead to improved results on many machine learning problems.\n",
        "\n",
        "Azure Machine Learning (AML) makes it easy to train, operate, and manage hundreds or even thousands of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split data in two sets\n",
        "\n",
        "Let's setup the path that holds the files for the individual airports and split the data into training and testing/inferencing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969513473
        }
      },
      "outputs": [],
      "source": [
        "from scripts.helper import split_data\n",
        "\n",
        "target_path = 'airports' \n",
        "os.makedirs(target_path, exist_ok=True)\n",
        "\n",
        "# Split each file and store in corresponding directory\n",
        "train_path, inference_path = split_data(target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload data to Datastore in AML Workspace\n",
        "\n",
        "Next step is to upload our splitted data into our Workspace default datastore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969514527
        }
      },
      "outputs": [],
      "source": [
        "# Connect to default datastore\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "# Upload train data\n",
        "ds_train_path = target_path + '_train'\n",
        "datastore.upload(src_dir=train_path, target_path=ds_train_path, overwrite=True)\n",
        "\n",
        "# Upload inference data\n",
        "ds_inference_path = target_path + '_inference'\n",
        "datastore.upload(src_dir=inference_path, target_path=ds_inference_path, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register dataset\n",
        "\n",
        "From our datastore lets register our new file dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969515729
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "# Create file datasets\n",
        "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\n",
        "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\n",
        "\n",
        "# Register the file datasets\n",
        "dataset_name = 'airport_data'\n",
        "train_dataset_name = dataset_name + '_train'\n",
        "inference_dataset_name = dataset_name + '_inference'\n",
        "ds_train.register(ws, train_dataset_name, create_new_version=True)\n",
        "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Experiment\n",
        "\n",
        "Afterwards lets setup a new Experiment for our Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969516506
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "experiment = Experiment(ws, 'manymodels-training-pipeline')\n",
        "\n",
        "print('Experiment name: ' + experiment.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch registered dataset\n",
        "\n",
        "Fetch the training and testing data from the Workspace Dataset registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969517442
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "\n",
        "filedst_5_models = Dataset.get_by_name(ws, name='airport_data_train')\n",
        "filedst_5_models_input = filedst_5_models.as_named_input('train_5_models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the training pipeline\n",
        "\n",
        "This dictionary defines the AutoML settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969518135
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from scripts.helper import write_automl_settings_to_file\n",
        "\n",
        "automl_settings = {\n",
        "    \"task\" : 'classification',\n",
        "    \"primary_metric\" : 'accuracy',\n",
        "    \"iteration_timeout_minutes\" : 30,\n",
        "    \"iterations\" : 25,\n",
        "    \"label_column_name\" : 'ArrDelay15',\n",
        "    \"verbosity\" : logging.INFO, \n",
        "    \"debug_log\": 'automl_many_models_debug.txt',\n",
        "    \"experiment_exit_score\": 0.8\n",
        "}\n",
        "\n",
        "write_automl_settings_to_file(automl_settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up environment  for ParallelRunStep\n",
        "\n",
        "\n",
        "Environment defines a collection of resources that we will need to run our pipelines. We configure a reproducible Python environment for our training script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969518831
        }
      },
      "outputs": [],
      "source": [
        "from scripts.helper import get_automl_environment\n",
        "train_env = get_automl_environment(workspace=ws, automl_settings_dict=automl_settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up ParallelRunConfig\n",
        "\n",
        "ParallelRunConfig is configuration for parallel run step. You will need to determine the number of workers and nodes appropriate for your use case. The process_count_per_node is based off the number of cores of the compute VM. The node_count will determine the number of master nodes to use, increasing the node count will speed up the training process.\n",
        "\n",
        "node_count: The number of compute nodes to be used for running the user script. We recommend to start with 3 and increase the node_count if the training time is taking too long.\n",
        "\n",
        "process_count_per_node: The number of processes per node.\n",
        "\n",
        "run_invocation_timeout: The run() method invocation timeout in seconds. The timeout should be set to maximum training time of one AutoML run(with some buffer), by default it's 60 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969519496
        }
      },
      "outputs": [],
      "source": [
        "from scripts.helper import build_parallel_run_config\n",
        "\n",
        "node_count=1\n",
        "process_count_per_node=6\n",
        "run_invocation_timeout=3700\n",
        "\n",
        "parallel_run_config = build_parallel_run_config(train_env, compute_target, node_count, process_count_per_node, run_invocation_timeout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Pipeline output path\n",
        "\n",
        "This ParallelRunStep is the main step in our pipeline. First, we set up the output directory and define the Pipeline's output name. The datastore that stores the pipeline's output data is Workspace's default datastore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969520157
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineData\n",
        "\n",
        "training_output_name = \"training_output\"\n",
        "\n",
        "output_dir = PipelineData(name=training_output_name, \n",
        "                          datastore=datastore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate ParallelRunStep\n",
        "\n",
        "We specify the following parameters:\n",
        "\n",
        "**name**: We set a name for our ParallelRunStep.\n",
        "\n",
        "**parallel_run_config**: We then pass the previously defined ParallelRunConfig.\n",
        "\n",
        "**allow_reuse**: Indicates whether the step should reuse previous results when re-run with the same settings.\n",
        "\n",
        "**inputs**: We are going to use the registered FileDataset that we called earlier in the Notebook. inputs points to a registered file dataset in AML studio that points to a path in the blob container. The number of files in that path determines the number of models will be trained in the ParallelRunStep.\n",
        "\n",
        "**output**: The output directory we just defined. A PipelineData object that corresponds to the output directory.\n",
        "\n",
        "**models**: Zero or more model names already registered in the Azure Machine Learning model registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969521085
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import ParallelRunStep\n",
        "\n",
        "parallel_run_step = ParallelRunStep(\n",
        "    name=\"many-models-training\",\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    allow_reuse = False,\n",
        "    inputs=[filedst_5_models_input], # train 10 models\n",
        "    output=output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit the pipeline to run\n",
        "\n",
        "Next we submit our pipeline to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626969527605
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.core.run import Run\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=parallel_run_step)\n",
        "run = experiment.submit(pipeline)\n",
        "RunDetails(run).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626971694285
        }
      },
      "outputs": [],
      "source": [
        "run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get list of AutoML runs along with registered model names and tags\n",
        "\n",
        "The following code snippet will iterate through all the automl runs for the experiment and list the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626971697112
        }
      },
      "outputs": [],
      "source": [
        "from scripts.helper import get_training_output\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "training_results_name = \"training_results\"\n",
        "\n",
        "training_file = get_training_output(run, training_results_name, training_output_name)\n",
        "all_columns = [\"Framework\", \"Dataset\", \"Run\", \"Status\", \"Model\", \"Tags\", \"StartTime\", \"EndTime\" , \"ErrorType\", \"ErrorCode\", \"ErrorMessage\" ]\n",
        "df = pd.read_csv(training_file, delimiter=\" \", header=None, names=all_columns)\n",
        "training_csv_file = \"training.csv\"\n",
        "df.to_csv(training_csv_file)\n",
        "df[['Framework','Dataset','Run','Status','Model','StartTime','EndTime']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Publish the pipeline\n",
        "\n",
        "Publish the pipeline for easy re-execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1626972232939
        }
      },
      "outputs": [],
      "source": [
        "published_pipeline = pipeline.publish(name = 'fd_train_many_models',\n",
        "                                      description = 'Flight Delay - train many models',\n",
        "                                      version = '1',\n",
        "                                      continue_on_step_failure = False)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
