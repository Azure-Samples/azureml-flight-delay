{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Flight Delay Demo - MLOps\n",
        "\n",
        "## Configure Datasheets\n",
        "\n",
        "Define helper functions to enable model data sheets."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from markdown import markdown\n",
        "\n",
        "def get_tag(tagname):\n",
        "    text = ''\n",
        "    try:\n",
        "        text = tags[tagname]\n",
        "    except:\n",
        "        print('Missing tag ' + tagname)\n",
        "    finally:\n",
        "        return text\n",
        "\n",
        "def get_datasheet(tags):\n",
        "    title = get_tag('title')\n",
        "    description = get_tag('datasheet_description')\n",
        "    details = get_tag('details')\n",
        "    date = get_tag('date')\n",
        "    modeltype = get_tag('type')\n",
        "    version = get_tag('version')\n",
        "    helpresources = get_tag('help')\n",
        "    usecase_primary = get_tag('usecase_primary')\n",
        "    usecase_secondary = get_tag('usecase_secondary')\n",
        "    usecase_outofscope = get_tag('usecase_outofscope')\n",
        "    dataset_description = get_tag('dataset_description')\n",
        "    motivation = get_tag('motivation')\n",
        "    caveats = get_tag('caveats')\n",
        "\n",
        "    datasheet = ''\n",
        "    datasheet+=markdown(f'# {title} \\n {description} \\n')\n",
        "    datasheet+=markdown(f'## Model Details \\n {details} \\n')\n",
        "    datasheet+=markdown(f'### Model date \\n {date} \\n')\n",
        "    datasheet+=markdown(f'### Model type \\n {modeltype} \\n')\n",
        "    datasheet+=markdown(f'### Model version \\n {version} \\n')\n",
        "    datasheet+=markdown(f'### Where to send questions or comments about the model \\n Please send questions or concerns using [{helpresources}]({helpresources}) \\n')\n",
        "    datasheet+=markdown('## Intended Uses:\\n')\n",
        "    datasheet+=markdown(f'### Primary use case \\n {usecase_primary} \\n')\n",
        "    datasheet+=markdown(f'### Secondary use case \\n {usecase_secondary} \\n')\n",
        "    datasheet+=markdown(f'### Out of scope \\n {usecase_outofscope} \\n')\n",
        "    datasheet+=markdown('## Evaluation Data:\\n')\n",
        "    datasheet+=markdown(f'### Datasets \\n {dataset_description} \\n')\n",
        "    datasheet+=markdown(f'### Motivation \\n {motivation} \\n')\n",
        "    datasheet+=markdown(f'### Caveats \\n {caveats} \\n')\n",
        "\n",
        "    return datasheet"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629235384484
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "import logging\r\n",
        "logging.basicConfig(level = logging.ERROR)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629235384715
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Workspace\n",
        "\n",
        "In the next cell, we create a new Workspace config object using the `<subscription_id>`, `<resource_group_name>`, and `<workspace_name>`. This will fetch the matching Workspace and prompt you for authentication. Please click on the link and input the provided details.\n",
        "\n",
        "For more information on **Workspace**, please visit: [Microsoft Workspace Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)\n",
        "\n",
        "`<subscription_id>` = You can get this ID from the landing page of your Resource Group.\n",
        "\n",
        "`<resource_group_name>` = This is the name of your Resource Group.\n",
        "\n",
        "`<workspace_name>` = This is the name of your Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.workspace import Workspace\r\n",
        "\r\n",
        "try:    \r\n",
        "    # Get instance of the Workspace and write it to config file\r\n",
        "    ws = Workspace(\r\n",
        "        subscription_id = '<subscription_id>', \r\n",
        "        resource_group = '<resource_group>', \r\n",
        "        workspace_name = '<workspace_name>')\r\n",
        "\r\n",
        "    # Writes workspace config file\r\n",
        "    ws.write_config()\r\n",
        "    \r\n",
        "    print('Library configuration succeeded')\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    print('Workspace not found')"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629235389749
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Drift\n",
        "\n",
        "Data drift is one of the top reasons model accuracy degrades over time. For machine learning models, data drift is the change in model input data that leads to model performance degradation. Monitoring data drift helps detect these model performance issues.\n",
        "\n",
        "Causes of data drift include:\n",
        "\n",
        "* Upstream process changes, such as a sensor being replaced that changes the units of measurement from inches to centimeters.\n",
        "* Data quality issues, such as a broken sensor always reading 0.\n",
        "* Natural drift in the data, such as mean temperature changing with the seasons.\n",
        "* Change in relation between features, or covariate shift."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "First step is to get our data using Dataset, the function `Dataset.get_by_name()` returns a registered Dataset from a given `workspace` and its registration `name`.\n",
        "\n",
        "`workspace` = The existing AzureML workspace in which the Dataset was registered..\n",
        "\n",
        "`name` = The registration name.\n",
        "\n",
        "`dataframe.take() ` = Function returns the elements in the given positional indices along an axis. \n",
        "\n",
        "For more information on **Dataset**, please visit: [Microsoft Dataset Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core import Dataset, Datastore\r\n",
        "\r\n",
        "tabular = Dataset.get_by_name(ws, 'flightdelayweather_ds')\r\n",
        "\r\n",
        "data = tabular.to_pandas_dataframe()\r\n",
        "tabular.take(3).to_pandas_dataframe()"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629235397971
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create AML Compute Cluster\n",
        "\n",
        "Firstly, check for the existence of the cluster. If it already exists, we are able to reuse it. Checking for the existence of the cluster can be performed by calling the constructor `ComputeTarget()` with the current workspace and name of the cluster.\n",
        "\n",
        "In case the cluster does not exist, the next step will be to provide a configuration for the new AML cluster by calling the function `AmlCompute.provisioning_configuration()`. It takes as parameters the VM size and the max number of nodes that the cluster can scale up to. After the configuration has executed, `ComputeTarget.create()` should be called with the previously configuration object and the workspace object.\n",
        "\n",
        "For more information on **ComputeTarget**, please visit: [Microsoft ComputeTarget Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py)\n",
        "\n",
        "For more information on **AmlCompute**, please visit: [Microsoft AmlCompute Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.akscompute?view=azure-ml-py)\n",
        "\n",
        "\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "### Create AML CPU Compute Cluster\r\n",
        "\r\n",
        "try:\r\n",
        "    compute_target = ComputeTarget(workspace=ws, name='cpucluster')\r\n",
        "    print('Found existing compute target.')\r\n",
        "except ComputeTargetException:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2',\r\n",
        "                                                           max_nodes=4)\r\n",
        "\r\n",
        "    # create the cluster\r\n",
        "    compute_target = ComputeTarget.create(ws, 'cpucluster', compute_config)\r\n",
        "\r\n",
        "    compute_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629235398849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create baseline for Data Drift Monitor\n",
        "\n",
        "Specify a baseline dataset - usually the training dataset for a model. A target dataset - usually model input data - is compared over time to your baseline dataset.\n",
        "\n",
        "The `from_delimited_files` creates a TabularDataset to represent tabular data in delimited files (e.g. CSV and TSV).\n",
        "\n",
        "The `with_timestamp_columns` defines timestamp columns for the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "data_drift = tabular.to_pandas_dataframe()\r\n",
        "data_drift.dropna()\r\n",
        "data_drift['Date'] = pd.to_datetime(dict(year=2008, month=data_drift.Month, day=data_drift.DayofMonth), errors='coerce')\r\n",
        "data_drift = data_drift[data_drift['Date'].notna()]\r\n",
        "file_name = 'flight_delay_ds_wDate.csv'\r\n",
        "data_drift.to_csv(file_name, index=False)\r\n",
        "data_store = Datastore.get_default(ws)\r\n",
        "data_store.upload_files(['./' + file_name], overwrite=True)\r\n",
        "datastore_path = [(data_store, file_name)]\r\n",
        "\r\n",
        "drift_tabular = Dataset.Tabular.from_delimited_files(datastore_path)\r\n",
        "\r\n",
        "# assign the timestamp attribute to a real or virtual column in the dataset\r\n",
        "drift_tabular = drift_tabular.with_timestamp_columns('Date')\r\n",
        "\r\n",
        "drift_tabular = drift_tabular.register(workspace=ws,\r\n",
        "                           name='target',\r\n",
        "                           create_new_version=True)\r\n",
        "\r\n",
        "drift_tabular.take(3).to_pandas_dataframe()"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629235406831
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Data Drift Monitor\n",
        "\n",
        "The DataDriftDetector class enables you to configure a data monitor object which then can be run as a job to analyze data drift. Data drift jobs can be run interactively or enabled to run on a schedule. \n",
        "\n",
        "The `get_by_name` retrieves a unique DataDriftDetector object for a given workspace and name.\n",
        "\n",
        "The `create_from_datasets` creates a new DataDriftDetector object from a baseline tabular dataset and a target time series dataset.\n",
        "\n",
        "For more information on **DataDriftDetector Class**, please visit: [Microsoft DataDriftDetector Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-datadrift/azureml.datadrift.datadriftdetector.datadriftdetector?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.datadrift import DataDriftDetector\n",
        "from datetime import datetime\n",
        "\n",
        "target = Dataset.get_by_name(ws, 'target')\n",
        "\n",
        "# set the baseline dataset\n",
        "baseline = target.time_before(datetime(2008, 4, 1))\n",
        "\n",
        "try:\n",
        "    # get data drift detector by name\n",
        "    monitor = DataDriftDetector.get_by_name(ws, 'fd-drift-monitor')\n",
        "except:\n",
        "    # set up data drift detector\n",
        "    monitor = DataDriftDetector.create_from_datasets(ws, 'fd-drift-monitor', baseline, target, \n",
        "                                                          compute_target=compute_target, \n",
        "                                                          frequency='Week', \n",
        "                                                          feature_list=None, \n",
        "                                                          drift_threshold=0.6, \n",
        "                                                          latency=24)\n",
        "\n",
        "\n",
        "\n",
        "columns  = list(baseline.take(1).to_pandas_dataframe())\n",
        "exclude  = ['Month', 'DayofMonth', 'DayofWeek','Origin_dayl', 'Dest_dayl', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp', '__index_level_0__']\n",
        "features = [col for col in columns if col not in exclude]\n",
        "\n",
        "# update data drift detector\n",
        "monitor = monitor.update(feature_list=features)\n",
        "\n",
        "backfill = monitor.backfill(datetime(2008, 4, 1), datetime(2008, 6, 1))\n",
        "\n",
        "backfill.wait_for_completion(show_output=False, wait_post_processing=True)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629236499071
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze historical data and backfill\n",
        "\n",
        "See how the dataset differs from the target dataset in the specified time period. The closer to 100%, the more the two datasets differ."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# get results from Python SDK (wait for backfills or monitor runs to finish)\r\n",
        "results, metrics = monitor.get_output(start_time=datetime(year=2008, month=4, day=1))\r\n",
        "# plot the results from Python SDK \r\n",
        "monitor.show(datetime(2008, 4, 1), datetime(2008, 6, 1))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1629236516128
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Register\n",
        "\n",
        "## Fetch latest model\n",
        "\n",
        "Let's fetch the latest run for our experiment."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.experiment import Experiment\r\n",
        "from azureml.train.automl.run import AutoMLRun\r\n",
        "\r\n",
        "experiment = Experiment(ws, 'flight-delay-exp')\r\n",
        "run = AutoMLRun(experiment, run_id=next(x for x in experiment.get_runs() if x.id.startswith('AutoML')).id)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236516392
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_run, fitted_model = run.get_output()\r\n",
        "print(best_run)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236520803
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Model\r\n",
        "Next, register the model obtained from the best run. In order to register the model, the function `register_model()` should be called. This will take care of registering the model obtained from the best run."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# register the model for deployment\r\n",
        "model = best_run.register_model(model_name='flight_delay_weather', \r\n",
        "                                model_path='outputs/model.pkl',\r\n",
        "                                datasets=[(Dataset.Scenario.TRAINING, tabular)],\r\n",
        "                                description='This model was developed by Microsoft to showcase the capabilities of Azure ML.',\r\n",
        "                       tags={'title': 'Flight Delay Model',\r\n",
        "    'datasheet_description':\r\n",
        "\"\"\"\r\n",
        "Last updated: October 2020\r\n",
        "\r\n",
        "Based on dataset from by [Statistical Computing Statistical Graphics](http://stat-computing.org/dataexpo/2009/the-data.html)\r\n",
        "\r\n",
        "\"\"\",\r\n",
        "    'details': 'This model was developed for Microsoft.',\r\n",
        "    'date': 'October 2020, trained on data that cuts off at the end of 2008.', \r\n",
        "    'type': 'Classification model',\r\n",
        "    'version': '1.0',\r\n",
        "    'help': 'https://www.azure.com/',\r\n",
        "    'usecase_primary': \r\n",
        "\"\"\"\r\n",
        "Developed for Flight Delay Demo.\r\n",
        "\r\n",
        "\"\"\",\r\n",
        "    'usecase_secondary':\r\n",
        "\"\"\"\r\n",
        "Field demos and marketing.\r\n",
        "\r\n",
        "\"\"\",\r\n",
        "    'usecase_outofscope':\r\n",
        "\"\"\"\r\n",
        "Do not use for production environments.\r\n",
        "\r\n",
        "\"\"\",\r\n",
        "    'dataset_description':\r\n",
        "\"\"\"\r\n",
        "The data comes originally from RITA where it is described in detail. You can download the data there, or from the bzipped csv files listed below. These files have derivable variables removed, are packaged in yearly chunks and have been more heavily compressed than the originals.\r\n",
        "\r\n",
        "\"\"\",\r\n",
        "    'motivation': 'Demo the main features behind the Azure ML Workspace environment',\r\n",
        "    'caveats':\r\n",
        "\"\"\"\r\n",
        "\"\"\"})\r\n",
        "\r\n",
        "print(\"Model name: \" + model.name, \"Model version: \" + str(model.version), sep=\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236521863
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traceability\r\n",
        "## Git hash \r\n",
        "We can fecth and display the associated git branch, commit and repository associated with the model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "from io import StringIO\r\n",
        "\r\n",
        "# Run details capture configuration and exact Git commit used for the run\r\n",
        "remote_run_df = pd.read_json(StringIO('[' + json.dumps(run.get_details()['properties']) + ']'), orient='columns')\r\n",
        "remote_run_df[['azureml.git.branch','azureml.git.commit','azureml.git.repository_uri']].T"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236522154
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace back to model run\r\n",
        "\r\n",
        "Run instance associated with the registered model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Trace back to the experiment\r\n",
        "model.run"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236522560
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace back to model dataset\r\n",
        "\r\n",
        "Dataset instance associated with the registered model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.DataFrame(\r\n",
        "    {'Dataset Id': model.datasets['training'][0].id,\r\n",
        "     'Name': model.datasets['training'][0].name }, index=[0])"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236522871
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Datasheet\r\n",
        "\r\n",
        "Datasheet associated with the registered model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython.core.display import display,Markdown\r\n",
        "\r\n",
        "tags = model.tags\r\n",
        "display(Markdown(get_datasheet(tags)))"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236523146
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-platform\n",
        "\n",
        "## Get ONNX model\n",
        "\n",
        "Open Neural Network Exchange (ONNX) can help optimize the inference of your machine learning model. Inference, or model scoring, is the phase where the deployed model is used for prediction, most commonly on production data.\n",
        "\n",
        "Microsoft and a community of partners created ONNX as an open standard for representing machine learning models. Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the standard ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices.\n",
        "\n",
        "With Azure Machine Learning, you can use automated ML to build a Python model and have it converted to the ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_run, onnx_mdl = run.get_output(return_onnx_model=True)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236539856
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save ONNX model\r\n",
        "\r\n",
        "Save ONNX Model to a local directory to be used for both cloud and edge and works on Linux, Windows, and Mac. Written in C++, it also has C, Python, C#, Java, and Javascript (Node.js) APIs for usage in a variety of environments."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.automl.runtime.onnx_convert import OnnxConverter\r\n",
        "\r\n",
        "onnx_fl_path = \"./best_model.onnx\"\r\n",
        "OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236540123
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OSS Model Export\r\n",
        "\r\n",
        "The `Model.get_model_path()` function returns the path to model.\r\n",
        "\r\n",
        "For more information on **Model Class**, please visit: [Microsoft Model Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import Model\r\n",
        "import joblib\r\n",
        "\r\n",
        "oss_model_path = Model.get_model_path(model_name = 'flight_delay_weather', _workspace=ws)\r\n",
        "\r\n",
        "# deserialize the model file back into a sklearn model\r\n",
        "oss_model = joblib.load(oss_model_path)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236540797
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OSS Model Prediction\r\n",
        "\r\n",
        "Let's try to perform a quick prediction over our OSS model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "training_data, validation_data = tabular.random_split(percentage=0.9, seed=1)\r\n",
        "le = LabelEncoder()\r\n",
        "Y = le.fit_transform(data['ArrDelay15'].values)\r\n",
        "val = validation_data.to_pandas_dataframe()\r\n",
        "val = val.drop(columns=['ArrDelay15'])\r\n",
        "X_train = data.drop(columns=['ArrDelay15'])\r\n",
        "X_test = data.drop(columns=['ArrDelay15'])\r\n",
        "Y_train = Y\r\n",
        "\r\n",
        "\r\n",
        "oss_model.predict(X_test.head(1))"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236542878
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register OSS Model\r\n",
        "\r\n",
        "Our next step would be to register the OSS model from our local directory.\r\n",
        "\r\n",
        "The `Model.register()` function registers a model with the provided workspace."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "oss_path = \"./oss_model/flight_delay_weather_.pkl\"\n",
        "# register the model for deployment\n",
        "oss_model = Model.register(workspace = ws,\n",
        "                            model_name='flight_delay_weather_', \n",
        "                            model_path=oss_path)\n",
        "\n",
        "print(\"Model name: \" + oss_model.name, \"Model version: \" + str(oss_model.version), sep=\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236543813
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretability\r\n",
        "\r\n",
        "Enabling the capability of explaining a machine learning model is important during two main phases of model development:\r\n",
        "\r\n",
        "* During the training phase, as model designers and evaluators can use interpretability output of a model to verify hypotheses and build trust with stakeholders. They also use the insights into the model for debugging, validating model behavior matches their objectives, and to check for model unfairness or insignificant features.\r\n",
        "\r\n",
        "* During the inferencing phase, as having transparency around deployed models empowers executives to understand \"when deployed\" how the model is working and how its decisions are treating and impacting people in real life.\r\n",
        "\r\n",
        "## Interpretability during inference \r\n",
        "\r\n",
        "Use `automl_setup_model_explanations` to get the engineered explanations. The `fitted_model` can generate the following items:\r\n",
        "\r\n",
        "* Featured data from trained or test samples\r\n",
        "* Engineered feature name lists\r\n",
        "* Findable classes in your labeled column in classification scenarios\r\n",
        "\r\n",
        "\r\n",
        "For more information on **Interpretability**, please visit: [Microsoft Interpretability Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-machine-learning-interpretability-automl)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\r\n",
        "\r\n",
        "model_2_path = Model.get_model_path(model_name = 'flight_delay_weather', _workspace=ws)\r\n",
        "model_2 = joblib.load(model_2_path)\r\n",
        "automl_explainer_setup_obj = automl_setup_model_explanations(model_2, X=X_train, \r\n",
        "                                                             X_test=val, y=Y_train, \r\n",
        "                                                             task='classification')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236670804
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate MimicWrapper and explain\r\n",
        "\r\n",
        "\r\n",
        "To generate an explanation for AutoML models, use the MimicWrapper class. You can initialize the MimicWrapper with these parameters:\r\n",
        "\r\n",
        "* The explainer setup object\r\n",
        "* Your workspace\r\n",
        "* A surrogate model to explain the fitted_model automated ML model\r\n",
        "\r\n",
        "The MimicWrapper also takes the automl_run object where the engineered explanations will be uploaded.\r\n",
        "\r\n",
        "For more information on **MimicWrapper Class**, please visit: [Microsoft MimicWrapper Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.mimicwrapper?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.interpret import MimicWrapper\r\n",
        "\r\n",
        "# Initialize the Mimic Explainer\r\n",
        "explainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator,\r\n",
        "                         explainable_model=automl_explainer_setup_obj.surrogate_model, \r\n",
        "                         init_dataset=automl_explainer_setup_obj.X_transform, run=best_run,\r\n",
        "                         features=automl_explainer_setup_obj.engineered_feature_names, \r\n",
        "                         feature_maps=[automl_explainer_setup_obj.feature_map],\r\n",
        "                         classes=automl_explainer_setup_obj.classes,\r\n",
        "                         explainer_kwargs=automl_explainer_setup_obj.surrogate_model_params)\r\n",
        "\r\n",
        "engineered_explanations = explainer.explain(['local', 'global'], eval_dataset=automl_explainer_setup_obj.X_test_transform)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236726819
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register scoring_explainer as model\r\n",
        "\r\n",
        "Use the `TreeScoringExplainer` to create the scoring explainer that'll compute the engineered feature importance values at inference time. You initialize the scoring explainer with the `feature_map` that was computed previously.\r\n",
        "\r\n",
        "For more information on **TreeScoringExplainer Class**, please visit: [Microsoft TreeScoringExplainer Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.scoring.scoring_explainer.treescoringexplainer?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\r\n",
        "\r\n",
        "# Initialize the ScoringExplainer\r\n",
        "scoring_explainer = TreeScoringExplainer(explainer.explainer, feature_maps=[automl_explainer_setup_obj.feature_map])\r\n",
        "\r\n",
        "# Pickle scoring explainer locally\r\n",
        "save(scoring_explainer, exist_ok=True)\r\n",
        "\r\n",
        "# Register scoring explainer\r\n",
        "try:\r\n",
        "    best_run.upload_file('scoring_explainer.pkl', 'scoring_explainer.pkl')\r\n",
        "except:\r\n",
        "    print('scoring_explainer.pkl already uploaded.')\r\n",
        "scoring_explainer_model = best_run.register_model(model_name='scoring_explainer', model_path='scoring_explainer.pkl')\r\n",
        "print(\"Model name: \" + scoring_explainer_model.name, \"Model version: \" + str(scoring_explainer_model.version), sep=\"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236728808
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical performance tracking\r\n",
        "\r\n",
        "## Create Scoring File\r\n",
        "\r\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome.\r\n",
        "\r\n",
        "The first thing to do in the scoring file is to fetch the model. This is done by calling `Model.get_model_path()` and passing the model name as a parameter.\r\n",
        "\r\n",
        "After the model has been loaded, the function `model.predict()` function should be called to start the scoring process.\r\n",
        "\r\n",
        "For more information on **Machine Learning - Score**, please visit: [Microsoft Machine Learning - Score Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-score)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile score.py\r\n",
        "import os\r\n",
        "import json\r\n",
        "import joblib\r\n",
        "import time\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import azureml.automl.core\r\n",
        "from azureml.core.model import Model\r\n",
        "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\r\n",
        "from inference_schema.schema_decorators import input_schema, output_schema\r\n",
        "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\r\n",
        "from azureml.monitoring import ModelDataCollector\r\n",
        " \r\n",
        "input_sample = np.array([[3,30,7,820,930,'MQ',70,'DFW','LIT',304,32.89595056,-97.0372,'TX',34.72939611,-92.22424556,'AR',44236.8,44236.8,0.0,11.0,409.6,208.0,0.0,0.0,28.5,16.0,15.0,8.5,1720.0,1120.0]])\r\n",
        "output_sample = np.array([1])\r\n",
        "model = None\r\n",
        "scoring_explainer = None\r\n",
        " \r\n",
        "def init():\r\n",
        "    global model, inputs_dc, prediction_dc, scoring_explainer\r\n",
        "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\r\n",
        "\r\n",
        "    try:\r\n",
        "        scoring_explainer_path = Model.get_model_path(\"scoring_explainer\")\r\n",
        "        scoring_explainer = joblib.load(scoring_explainer_path)\r\n",
        "        model_path = Model.get_model_path(\"flight_delay_weather\")\r\n",
        "    except:\r\n",
        "        model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), 'model.pkl')\r\n",
        "\r\n",
        "    model = joblib.load(model_path)\r\n",
        "    inputs_dc = ModelDataCollector('flight_delay', designation='inputs', feature_names=['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp'])\r\n",
        "    prediction_dc = ModelDataCollector('flight_delay', designation='predictions', feature_names=['ArrDelay15'])\r\n",
        "    \r\n",
        "@input_schema('data', NumpyParameterType(input_sample))\r\n",
        "@output_schema(NumpyParameterType(output_sample))\r\n",
        "def run(data):\r\n",
        "    try:\r\n",
        "        df = pd.DataFrame(data, columns=['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp']) \r\n",
        "        result = model.predict(df)\r\n",
        "        if scoring_explainer:\r\n",
        "            automl_explainer_setup_obj = automl_setup_model_explanations(model, X_test=df, task='classification')\r\n",
        "            local_importance_values = scoring_explainer.explain(automl_explainer_setup_obj.X_test_transform, get_raw=True)\r\n",
        "        else:\r\n",
        "            local_importance_values = []\r\n",
        "        inputs_dc.collect(data) #this call is saving our input data into Azure Blob\r\n",
        "        prediction_dc.collect(result) #this call is saving our output data into Azure Blob\r\n",
        "    except Exception as e:\r\n",
        "        result = str(e)\r\n",
        "        print(result)\r\n",
        "        return {\"error\": result}\r\n",
        "    return {\"result\": result.tolist(), 'local_importance_values': local_importance_values}"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Dataset for Model Profiling\r\n",
        "\r\n",
        "The next cell will create a JSON File from our registered training dataset. The purpose of the file is to create a load for the service in order to achieve an adequate profiling. The profiling dataset will be registered under the name `sample_request_data`.\r\n",
        "\r\n",
        "The Datastore Class represents a storage abstraction over an Azure Machine Learning storage account.\r\n",
        "The `get_default` function is used to get the default datastore for the workspace.\r\n",
        "\r\n",
        "The `dataset_type_definitions` Module contains enumeration values used with Dataset.\r\n",
        "The `PromoteHeadersBehavior` function defines options for how column headers are processed when reading data from files to create a dataset.\r\n",
        "\r\n",
        "For more information on **Datastore Class**, please visit: [Microsoft Machine Learning - Datastore Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **dataset_type_definitions **, please visit: [Microsoft Machine Learning - dataset_type_definitions Module Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.dataset_type_definitions?view=azure-ml-py)\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "from azureml.core import Datastore\r\n",
        "from azureml.data import dataset_type_definitions\r\n",
        "\r\n",
        "sample = val.head(10).values.tolist()\r\n",
        "serialized_input_json = json.dumps({'data': sample})\r\n",
        "\r\n",
        "dataset_content = []\r\n",
        "for i in range(100):\r\n",
        "    dataset_content.append(serialized_input_json)\r\n",
        "dataset_content = '\\n'.join(dataset_content)\r\n",
        "file_name = 'sample_request_data.txt'\r\n",
        "f = open(file_name, 'w')\r\n",
        "f.write(dataset_content)\r\n",
        "f.close()\r\n",
        "\r\n",
        "# upload the txt file created above to the Datastore and create a dataset from it\r\n",
        "data_store = Datastore.get_default(ws)\r\n",
        "data_store.upload_files(['./' + file_name], target_path='sample_request_data', overwrite=True)\r\n",
        "datastore_path = [(data_store, 'sample_request_data' +'/' + file_name)]\r\n",
        "sample_request_data = Dataset.Tabular.from_delimited_files(\r\n",
        "    datastore_path, separator='\\n',\r\n",
        "    infer_column_types=True,\r\n",
        "    header=dataset_type_definitions.PromoteHeadersBehavior.NO_HEADERS)\r\n",
        "sample_request_data = sample_request_data.register(workspace=ws,\r\n",
        "                                                   name='sample_request_data',\r\n",
        "                                                   create_new_version=True)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629236730800
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profile model\r\n",
        "\r\n",
        "Profiles the model to get resource requirement recommendations.\r\n",
        "\r\n",
        "Profiling will determine the CPU and memory the deployed service will need. Profiling tests the service that runs your model and returns information such as the CPU usage, memory usage, and response latency. It also provides a recommendation for the CPU and memory based on resource usage.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import InferenceConfig\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "from azureml.core.webservice import AksWebservice\r\n",
        "from azureml.core.profile import ModelProfile\r\n",
        "from azureml.core.model import Model\r\n",
        "from azureml.core import Environment\r\n",
        "import time\r\n",
        "\r\n",
        "ts = str(int(time.time()))\r\n",
        "model_profile_name = \"fd_model_\" + ts\r\n",
        "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.22.1'],pip_packages=['azureml-sdk[notebooks,automl]','azureml-defaults','inference-schema','azureml-monitoring','shap==0.39.0'])\r\n",
        "\r\n",
        "with open(\"score-new.yml\",\"w\") as f:\r\n",
        "    f.write(myenv.serialize_to_string())\r\n",
        "\r\n",
        "myenv = Environment.from_conda_specification(name = \"myenv\",\r\n",
        "                                             file_path = \"score-new.yml\")\r\n",
        "\r\n",
        "# Create an inference config object based on the score.py and myenv.yml from previous steps\r\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\",\r\n",
        "                                    environment=myenv)\r\n",
        "\r\n",
        "\r\n",
        "input_dataset = Dataset.get_by_name(workspace=ws, name='sample_request_data')\r\n",
        "\r\n",
        "profile = Model.profile(ws, model_profile_name, [model, scoring_explainer_model], inference_config, input_dataset=input_dataset)\r\n",
        "profile.wait_for_completion(True)\r\n",
        "\r\n",
        "profiling_details = profile.get_details()\r\n",
        "profiling_details = pd.DataFrame.from_dict(profiling_details, orient='index', columns=['Profiling Result'])\r\n",
        "profiling_details"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237801942
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment\n",
        "\n",
        "## Deploy to Managed Endpoints\n",
        "\n",
        "Create a new directory to hold the configuration files for deploying a managed endpoint."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "\n",
        "managed_endpoints = './managed-endpoints'\n",
        "\n",
        "# Working directory\n",
        "if not os.path.exists(managed_endpoints):\n",
        "    os.makedirs(managed_endpoints)\n",
        "    \n",
        "if os.path.exists(os.path.join(managed_endpoints,\".amlignore\")):\n",
        "  os.remove(os.path.join(managed_endpoints,\".amlignore\"))"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238593925
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Scoring File\n",
        "\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!cp ./score.py $managed_endpoints"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the environment definition\r\n",
        "\r\n",
        "The following file contains the details of the environment to host the model and code. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/score-new.yml\n",
        "name: mlops-model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - numpy\n",
        "  - pip\n",
        "  - scikit-learn==0.22.1\n",
        "  - scipy\n",
        "  - pip:\n",
        "    - azureml-defaults\n",
        "    - azureml-sdk[notebooks,automl]\n",
        "    - pandas\n",
        "    - inference-schema[numpy-support]\n",
        "    - joblib\n",
        "    - numpy\n",
        "    - scipy\n",
        "    - azureml-monitoring\n",
        "    - shap==0.39.0"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the endpoint configuration\r\n",
        "Specific inputs are required to deploy a model on an online endpoint:\r\n",
        "\r\n",
        "1. Model files.\r\n",
        "1. The code that's required to score the model.\r\n",
        "1. An environment in which your model runs.\r\n",
        "1. Settings to specify the instance type and scaling capacity."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/endpointconfig.yml\r\n",
        "name: fd-mlops-endpoint\r\n",
        "type: online\r\n",
        "auth_mode: key\r\n",
        "traffic:\r\n",
        "  blue: 100\r\n",
        "\r\n",
        "deployments:\r\n",
        "  #blue deployment\r\n",
        "  - name: blue\r\n",
        "    model: azureml:flight_delay_weather:1\r\n",
        "    code_configuration:\r\n",
        "      code:\r\n",
        "        local_path: ./\r\n",
        "      scoring_script: score.py\r\n",
        "    environment: \r\n",
        "      name: fd-mlops-env\r\n",
        "      version: 1\r\n",
        "      path: ./\r\n",
        "      conda_file: file:./score-new.yml\r\n",
        "      docker:\r\n",
        "          image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\r\n",
        "    instance_type: Standard_DS3_v2\r\n",
        "    scale_settings:\r\n",
        "      scale_type: manual\r\n",
        "      instance_count: 1\r\n",
        "      min_instances: 1\r\n",
        "      max_instances: 2"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy your managed online endpoint to Azure\r\n",
        "\r\n",
        "This deployment might take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment will finish processing more quickly."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint create -g [your resource group name] -w [your AML workspace name] -n fd-mlops-mng-endpoint -f ./managed-endpoints/endpointconfig.yml"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a sample request JSON file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/sample-request.json\n",
        "{\"data\": [\n",
        "[6.0,21.0,6.0,1330.0,1600.0,9.0,150.0,16.0,93.0,745.0,33.64044444,-84.42694444,8.0,40.69249722,-74.16866056,29.0,51148.8,53568.0,2.0,0.0,438.4,451.2,0.0,0.0,30.5,28.5,18.0,15.0,2040.0,1720.0],\n",
        "[4.0,2.0,3.0,1910.0,2035.0,11.0,85.0,222.0,62.0,361.0,35.87763889,-78.78747222,25.0,39.99798528,-82.89188278,33.0,44928.0,45273.6,0.0,0.0,355.2,438.4,0.0,0.0,23.0,12.5,12.0,1.5,1400.0,680.0],\n",
        "[1.0,3.0,4.0,935.0,1224.0,16.0,229.0,207.0,78.0,1302.0,39.87195278,-75.24114083,36.0,32.89595056,-97.0372,41.0,33177.6,35596.8,0.0,0.0,156.8,252.8,0.0,0.0,-2.0,6.5,-8.0,-4.5,320.0,440.0],\n",
        "[4.0,3.0,4.0,1000.0,1252.0,16.0,172.0,207.0,206.0,951.0,39.87195278,-75.24114083,36.0,26.68316194,-80.09559417,7.0,45273.6,44582.4,0.0,4.0,425.6,220.8,0.0,0.0,12.0,28.0,0.5,22.5,640.0,2720.0],\n",
        "[1.0,21.0,1.0,800.0,1045.0,15.0,105.0,198.0,129.0,589.0,41.979595,-87.90446417,12.0,38.94453194,-77.45580972,43.0,33868.8,34905.6,2.0,0.0,256.0,246.4,56.0,0.0,-7.0,-3.0,-17.0,-13.5,160.0,200.0],\n",
        "[3.0,12.0,3.0,1640.0,1952.0,5.0,192.0,89.0,101.0,1065.0,40.69249722,-74.16866056,29.0,26.07258333,-80.15275,7.0,41817.6,42508.8,0.0,0.0,336.0,368.0,0.0,0.0,10.0,27.0,0.5,19.5,640.0,2280.0],\n",
        "[3.0,19.0,3.0,1229.0,1346.0,6.0,77.0,151.0,76.0,214.0,40.77724306,-73.87260917,32.0,38.85208333,-77.03772222,43.0,42854.4,42854.4,22.0,0.0,204.8,307.2,0.0,0.0,10.0,15.0,4.0,6.5,800.0,960.0],\n",
        "[4.0,18.0,5.0,1210.0,1503.0,4.0,173.0,139.0,169.0,944.0,40.63975111,-73.77892556,32.0,28.42888889,-81.31602778,7.0,47692.8,45964.8,0.0,0.0,524.8,508.8,0.0,0.0,22.5,26.0,5.5,11.5,920.0,1360.0],\n",
        "[11.0,1.0,6.0,615.0,745.0,9.0,90.0,130.0,18.0,432.0,39.71732917,-86.29438417,13.0,33.64044444,-84.42694444,8.0,36633.6,38016.0,0.0,0.0,297.6,387.2,0.0,0.0,22.0,20.5,5.0,2.0,880.0,720.0],\n",
        "[11.0,24.0,1.0,936.0,1123.0,8.0,107.0,208.0,77.0,602.0,33.43416667,-112.00805559999999,3.0,39.85840806,-104.6670019,5.0,35942.4,34214.4,0.0,0.0,297.6,291.2,0.0,0.0,27.5,17.0,10.0,-9.5,520.0,280.0]]}"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoke the endpoint to score data by using your model\r\n",
        "\r\n",
        "You can use either the invoke command or a REST client of your choice to invoke the endpoint and score against it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint invoke -g [your resource group name] -w [your AML workspace name] -n fd-mlops-mng-endpoint --request-file ./managed-endpoints/sample-request.json"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create/connect to the Kubernetes compute cluster\n",
        "\n",
        "The `AksCompute Class` manages an Azure Kubernetes Service compute target in Azure Machine Learning.\n",
        "\n",
        "The `ComputeTarget Class` is an abstract parent class for all compute targets managed by Azure Machine Learning. A compute target is a designated compute resource/environment where you run your training script or host your service deployment. \n",
        "\n",
        "The `ComputeTargetException` is an exception related to failures when creating, interacting with, or configuring a compute target. This exception is commonly raised for failures attaching a compute target, missing headers, and unsupported configuration values.\n",
        "\n",
        "For more information on **AksCompute Class**, please visit: [Microsoft Machine Learning - AksCompute Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py)\n",
        "\n",
        "For more information on **ComputeTarget Class**, please visit: [Microsoft Machine Learning - ComputeTarget Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py)\n",
        "\n",
        "For more information on **ComputeTargetException Class**, please visit: [Microsoft Machine Learning - ComputeTargetException Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.exceptions.computetargetexception?view=azure-ml-py)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.compute import AksCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "\n",
        "prov_config = AksCompute.provisioning_configuration(location='westus2')\n",
        "\n",
        "try:\n",
        "    aks_target = AksCompute(ws, 'flight-delay-aks')\n",
        "except ComputeTargetException:\n",
        "    # Create the cluster\n",
        "    aks_target = ComputeTarget.create(workspace = ws, \n",
        "                            name = 'flight-delay-aks', \n",
        "                            provisioning_configuration = prov_config)\n",
        "    aks_target.wait_for_completion(True)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237802459
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model to Kubernetes\r\n",
        "\r\n",
        "The first step is to define the dependencies that are needed for the service to run and they are defined by calling `CondaDependencies.create()`. This create function will receive as parameters the pip and conda packages to install on the remote machine. Secondly, the output of this function is persisted into a `.yml` file that will be leveraged later on the process.\r\n",
        "\r\n",
        "Now that the AKS cluster has been deployed and our CondaDependencies have been declared, its time to create an `InferenceConfig` object by calling its constructor and passing the runtime type, the path to the `entry_script` (score.py), and the `conda_file` (the previously created file that holds the environment dependencies).\r\n",
        "\r\n",
        "Next, define the configuration of the web service to deploy. This is done by calling `AksWebservice.deploy_configuration()` and passing along the number of `cpu_cores` and `memory_gb` that the service needs.\r\n",
        "\r\n",
        "Finally, in order to deploy the model and service to the created AKS cluster, the function `Model.deploy()` should be called, passing along the workspace object, a list of models to deploy, the defined inference configuration, deployment configuration, and the AKS object created in the step above.\r\n",
        "\r\n",
        "For more information on **CondaDependencies**, please visit: [Microsoft CondaDependencies Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **InferenceConfig**, please visit: [Microsoft InferenceConfig Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **AksWebService**, please visit: [Microsoft AksWebService Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **Model**, please visit: [Microsoft Model Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)\r\n",
        "\r\n",
        "\r\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.exceptions import WebserviceException\n",
        "\n",
        "inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                    entry_script=\"score.py\",\n",
        "                                    conda_file=\"score-new.yml\")\n",
        "\n",
        "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                                        memory_gb = 1,\n",
        "                                                        collect_model_data=True, \n",
        "                                                        enable_app_insights=True)\n",
        "\n",
        "try:\n",
        "    service = AksWebservice(ws, 'flight-delay-aml')\n",
        "    print(service.state)\n",
        "except WebserviceException:\n",
        "    service = Model.deploy(ws, \n",
        "                            'flight-delay-aml', \n",
        "                            [model, scoring_explainer_model], \n",
        "                            inference_config, \n",
        "                            deployment_config, \n",
        "                            aks_target)\n",
        "\n",
        "    service.wait_for_deployment(show_output = True)\n",
        "    print(service.state)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237806056
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to deployed webservice\n",
        "\n",
        "Now with test data, we can get it into a suitable format to consume the web service. First an instance of the web service should be obtained by calling the constructor `Webservice()` with the Workspace object and the service name as parameters. Sanitizing of the data is then performed in order to avoid sending unexpected columns to the web service. Finally, call the service via POST using the `requests` module. `requests.post()` will call the deployed web service. It takes for parameters the service URL, the test data, and a headers dictionary that contains the authentication token.\n",
        "\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "from azureml.core.webservice import Webservice\n",
        "\n",
        "aks_service = Webservice(ws, 'flight-delay-aml')\n",
        "\n",
        "# prepare the test data\n",
        "sample = val.sample(n=10, random_state=4).values.tolist()\n",
        "\n",
        "headers = {'Content-Type':'application/json'}\n",
        "\n",
        "if aks_service.auth_enabled:\n",
        "    headers['Authorization'] = 'Bearer '+ aks_service.get_keys()[0]\n",
        "\n",
        "output_df = []\n",
        "for x in sample:    \n",
        "    test_sample = json.dumps({'data': [x]})\n",
        "    response = requests.post(aks_service.scoring_uri, data=test_sample, headers=headers)\n",
        "    prediction = [response.json()['result'][0]]\n",
        "    prediction.extend(x)\n",
        "    output_df.append(prediction)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237816913
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Present scoring service predictions\r\n",
        "\r\n",
        "Let's format our service responses and present them in a suitable way to our end users."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def highlight_delays(val):\n",
        "    return 'background-color: yellow' if val == True else ''\n",
        "\n",
        "predictions = pd.DataFrame(output_df, columns =['Prediction', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp'])\n",
        "predictions = predictions.style.applymap(highlight_delays, subset=['Prediction'])\n",
        "predictions"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237818264
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretability at inference time\r\n",
        "\r\n",
        "Let's observe the top important features that lead our results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "inference_interpretability = pd.DataFrame(response.json()['local_importance_values'], columns =['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp']).T\n",
        "inference_interpretability.columns = ['Importance']\n",
        "inference_interpretability = inference_interpretability.sort_values(by=['Importance'], ascending=False)\n",
        "inference_interpretability.head(8)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629237818701
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Docker Package\n",
        "\n",
        "Create a model package in the form of a Docker image or Dockerfile build context. When deploying to Azure ML inferencing compute, this is done transparently; however, for other hosting targets, the Docker image can be built and exported automatically."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "docker_package = Model.package(ws, \n",
        "                                [model, scoring_explainer_model], \n",
        "                                inference_config)\n",
        "\n",
        "docker_package.wait_for_creation(show_output=True)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238324990
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Security\r\n",
        "## Homomorphic Encryption\r\n",
        "\r\n",
        "The encryption method used in this sample is homomorphic encryption. Homomorphic encryption allows for computations to be done on encrypted data without requiring access to a secret (decryption) key. The results of the computations are encrypted and can be revealed only by the owner of the secret key. \r\n",
        "\r\n",
        "The `BlobServiceClient` Class is a client to interact with the Blob Service at the account level.\r\n",
        "This client provides operations to retrieve and configure the account properties as well as list, create and delete containers within the account. We pass a connection string to the `from_connection_string` function to create the BlobServiceClient.\r\n",
        "\r\n",
        "\r\n",
        "For more information on **BlobServiceClient**, please visit: [Microsoft BlobServiceClient Documentation](https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile hom_score.py\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import azureml.train.automl\n",
        "from azureml.core.model import Model\n",
        "import joblib\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from encrypted.inference.eiserver import EIServer\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
        "    \n",
        "    # this name is model.id of model that we want to deploy\n",
        "    model_path = Model.get_model_path(model_name = 'flight_delay_weather_')\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    global server\n",
        "    server = EIServer(model.coef_, model.intercept_, verbose=True)\n",
        "\n",
        "def run(raw_data):\n",
        "\n",
        "    json_properties = json.loads(raw_data)\n",
        "\n",
        "    key_id = json_properties['key_id']\n",
        "    conn_str = json_properties['conn_str']\n",
        "    container = json_properties['container']\n",
        "    data = json_properties['data']\n",
        "\n",
        "    # download the Galois keys from blob storage \n",
        "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container, blob=key_id)\n",
        "    public_keys = blob_client.download_blob().readall()\n",
        "    \n",
        "    result = {}\n",
        "    # make prediction\n",
        "    result = server.predict(data, public_keys)\n",
        "\n",
        "    # you can return any data type as long as it is JSON-serializable\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homomorphic Encryption Dependencies\r\n",
        "\r\n",
        "Create an environment for inferencing and add encrypted-inference package as a conda dependency."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import InferenceConfig, Model\n",
        "from azureml.core.dataset import Dataset\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "azureml_pip_packages = ['azureml-defaults', 'azureml-contrib-interpret', 'azureml-core', 'azureml-telemetry',\n",
        "                        'azureml-train-automl', 'azureml-interpret', 'azureml-dataprep','azureml-dataprep[fuse,pandas]','joblib',\n",
        "                        'matplotlib','scikit-learn==0.22.1','seaborn','fairlearn','encrypted-inference==0.9','azure-storage-blob']\n",
        "\n",
        "# Define dependencies needed in the remote environment\n",
        "hom_myenv = CondaDependencies.create(pip_packages=azureml_pip_packages)\n",
        "\n",
        "# Write dependencies to yml file\n",
        "with open(\"hom_myenv.yml\",\"w\") as f:\n",
        "    f.write(hom_myenv.serialize_to_string())\n",
        "\n",
        "# Create an inference config object based on the score.py and myenv.yml from previous steps\n",
        "inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                    entry_script=\"hom_score.py\",\n",
        "                                    conda_file=\"hom_myenv.yml\")"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238326694
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy Homomorphic Encryption service to AKS\n",
        "\n",
        "The process of deploying the HE-enabled service to AKS is no different to other services."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                                        memory_gb = 1,\n",
        "                                                        enable_app_insights=True)\n",
        "\n",
        "model = Model(ws, name='flight_delay_weather_')\n",
        "\n",
        "try:\n",
        "    service = AksWebservice(ws, 'flight-delay-aml-hom')\n",
        "    print(service.state)\n",
        "except WebserviceException:\n",
        "    service = Model.deploy(ws, \n",
        "                        'flight-delay-aml-hom', \n",
        "                        [model], \n",
        "                        inference_config, \n",
        "                        deployment_config, \n",
        "                        aks_target)\n",
        "\n",
        "    service.wait_for_deployment(show_output = True)\n",
        "    print(service.state)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238327010
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create public and private keys\r\n",
        "\r\n",
        "In order to work with Homomorphic Encryption we need to generate our private and public keys to satisfy the encryption process."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore\n",
        "from encrypted.inference.eiclient import EILinearRegressionClient\n",
        "\n",
        "# Create a new Encrypted inference client and a new secret key.\n",
        "edp = EILinearRegressionClient(verbose=True)\n",
        "\n",
        "public_keys_blob, public_keys_data = edp.get_public_keys()\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "container_name = datastore.container_name\n",
        "\n",
        "# Create a local file and write the keys to it\n",
        "public_keys = open(public_keys_blob, \"wb\")\n",
        "public_keys.write(public_keys_data)\n",
        "public_keys.close()\n",
        "\n",
        "# Upload the file to blob store\n",
        "datastore.upload_files([public_keys_blob])\n",
        "\n",
        "# Delete the local file\n",
        "os.remove(public_keys_blob)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238330849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect raw data\r\n",
        "\r\n",
        "Let's observe how our raw data looks before we encrypt it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_test_hom = Dataset.get_by_name(ws, 'flightdelayweather_ds_clean')\n",
        "X_test_hom = X_test_hom.to_pandas_dataframe().drop(columns=['ArrDelay15'])\n",
        "\n",
        "sample_index = 0\n",
        "X_test_hom.iloc[sample_index].to_frame()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238331890
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect encrypted data\r\n",
        "\r\n",
        "Let's observe how our encrypted data looks like."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sample_data = (X_test_hom.to_numpy())\n",
        "raw_data = edp.encrypt(sample_data[sample_index])"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238332227
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Service with Encrypted data\r\n",
        "\r\n",
        "Now with test data, we can get it into a suitable format to consume the web service. First an instance of the web service should be obtained by calling the constructor `Webservice()` with the Workspace object and the service name as parameters. \r\n",
        "\r\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "from azureml.core import Webservice\n",
        "\n",
        "service = Webservice(ws, 'flight-delay-aml-hom')\n",
        "service.update(enable_app_insights=True)\n",
        "\n",
        "#pass the connection string for blob storage to give the server access to the uploaded public keys \n",
        "conn_str_template = 'DefaultEndpointsProtocol={};AccountName={};AccountKey={};EndpointSuffix=core.windows.net'\n",
        "conn_str = conn_str_template.format(datastore.protocol, datastore.account_name, datastore.account_key)\n",
        "\n",
        "#build the json \n",
        "data = json.dumps({\"data\": raw_data, \"key_id\" : public_keys_blob, \"conn_str\" : conn_str, \"container\" : container_name })\n",
        "data = bytes(data, encoding='ASCII')\n",
        "\n",
        "print ('Making an encrypted inference web service call ')\n",
        "eresult = service.run(input_data=data)\n",
        "\n",
        "print ('Received encrypted inference results')\n",
        "print (f'Encrypted results: ...', eresult[0][0:100], '...')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1629238340956
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decrypting Service Response\r\n",
        "\r\n",
        "The below cell uses the `decrypt()` function to decrypt the response from the deployed AKS Service. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np \n",
        "\n",
        "results = edp.decrypt(eresult)\n",
        "\n",
        "prediction = 'On-Time'\n",
        "if results[0] > 0:\n",
        "    prediction = 'Delayed'\n",
        "\n",
        "print ( ' Prediction : ', prediction)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
