{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Flight Delay Demo - Classical ML with VS Code\n",
        "\n",
        "## Install prerequisites\n",
        "\n",
        "Before running the notebook, make sure the correct versions of these libraries are installed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install --upgrade azureml-sdk azureml-widgets azureml-train-automl scipy azureml-contrib-notebook azureml-automl-core azureml-explain-model azureml-opendatasets azureml-train-automl-runtime pandas"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az extension add --name ml"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup working directory\n",
        "\n",
        "The cell below creates our working directory. This will hold our generated scripts."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "\r\n",
        "project_folder = './scripts'\r\n",
        "\r\n",
        "# Working directory\r\n",
        "if not os.path.exists(project_folder):\r\n",
        "    os.makedirs(project_folder)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write helper files\n",
        "\n",
        "The `sklearn.preprocessing.LabelEncoder` encodes target labels with value between 0 and n_classes-1.\n",
        "\n",
        "The `sklearn.model_selection.train_test_split` splits arrays or matrices into random train and test subsets\n",
        "\n",
        "The `sklearn.metrics.accuracy_score` is an accuracy classification score. In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
        "\n",
        "The `sklearn.metrics.confusion_matrix` is compute confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "The `sklearn.metrics.f1_score` computes the F1 score, also known as balanced F-score or F-measure. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The `sklearn.metrics.precision_score` computes the precision. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives.\n",
        "\n",
        "The `sklearn.metrics.recall_score` computes the recall. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The `sklearn.metrics.roc_auc_score` computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
        "\n",
        "The `sklearn.metrics.roc_curve` computes Receiver operating characteristic (ROC).\n",
        "\n",
        "The `Model Class` represents the result of machine learning training. A model is the result of a Azure Machine learning training Run or some other model training process outside of Azure. Regardless of how the model is produced, it can be registered in a workspace, where it is represented by a name and a version. \n",
        "\n",
        "\n",
        "For more information on **Model Class**, please visit: [Microsoft Model Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $project_folder/utils.py\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from azureml.core import Model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
        "from azureml.core import Model\n",
        "\n",
        "def split_dataset(X_raw, Y):\n",
        "    A = X_raw[['UniqueCarrier']]\n",
        "    X = X_raw.drop(labels=['UniqueCarrier'],axis = 1)\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(Y)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_raw, \n",
        "                                                        Y, \n",
        "                                                        A,\n",
        "                                                        test_size = 0.2,\n",
        "                                                        random_state=123,\n",
        "                                                        stratify=Y)\n",
        "\n",
        "    # Work around indexing bug\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    A_train = A_train.reset_index(drop=True)\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    A_test = A_test.reset_index(drop=True)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test, A_train, A_test \n",
        "\n",
        "def prepareDataset(X_raw):\n",
        "    df = X_raw\n",
        "    Y = df['ArrDelay15'].values\n",
        "    synth_df = df.drop(columns=['ArrDelay15'])\n",
        "    return synth_df, Y\n",
        "\n",
        "def register_model(name, model, ws):\n",
        "    print(\"Registering \", name)\n",
        "    joblib.dump(value=model, filename=\"models/flight_delay_weather_.pkl\")\n",
        "    registered_model = Model.register(model_path=\"models/flight_delay_weather_.pkl\",\n",
        "                                      model_name='flight_delay_weather_',\n",
        "                                      workspace=ws)\n",
        "    print(\"Registered \", registered_model.id)\n",
        "    return registered_model.id\n",
        "\n",
        "def analyze_model(clf, X_test, Y_test, preds):\n",
        "    accuracy = accuracy_score(Y_test, preds)\n",
        "    print(f'Accuracy', np.float(accuracy))\n",
        "\n",
        "    precision = precision_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'Precision', np.float(precision))\n",
        "\n",
        "    recall = recall_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'Recall', np.float(recall))\n",
        "\n",
        "    f1score = f1_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'F1 Score', np.float(f1score))\n",
        "\n",
        "    class_names = clf.classes_\n",
        "    fig, ax = plt.subplots()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    sns.heatmap(pd.DataFrame(confusion_matrix(Y_test, preds)), annot=True, cmap='YlGnBu', fmt='g')\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    plt.tight_layout()\n",
        "    plt.title('Confusion Matrix', y=1.1)\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    preds_proba = clf.predict_proba(X_test)[::,1]\n",
        "    fpr, tpr, _ = roc_curve(Y_test, preds_proba, pos_label = clf.classes_[1])\n",
        "    auc = roc_auc_score(Y_test, preds_proba)\n",
        "    plt.plot(fpr, tpr, label=\"data 1, auc=\" + str(auc))\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Workspace\n",
        "\n",
        "In the next cell, we create a new Workspace config object using the `<subscription_id>`, `<resource_group_name>`, and `<workspace_name>`. This will fetch the matching Workspace and prompt you for authentication. Please click on the link and input the provided details.\n",
        "\n",
        "For more information on **Workspace**, please visit: [Microsoft Workspace Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)\n",
        "\n",
        "`<subscription_id>` = You can get this ID from the landing page of your Resource Group.\n",
        "\n",
        "`<resource_group_name>` = This is the name of your Resource Group.\n",
        "\n",
        "`<workspace_name>` = This is the name of your Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.workspace import Workspace\r\n",
        "\r\n",
        "try:\r\n",
        "    # Get instance of the Workspace and write it to config file\r\n",
        "    ws = Workspace(\r\n",
        "        subscription_id = '<subscription_id>', \r\n",
        "        resource_group = '<resource_group>', \r\n",
        "        workspace_name = '<workspace_name>')\r\n",
        "\r\n",
        "    # Writes workspace config file\r\n",
        "    ws.write_config()\r\n",
        "    \r\n",
        "    print('Library configuration succeeded')\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    print('Workspace not found')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "\n",
        "Pandas `read_csv()` function is used to read local file which containes our data set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "data = pd.read_csv('./flightdelayweather_ds_clean.csv')"
      ],
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration and visualization"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observe training dataset\n",
        "\n",
        "`pandas.DataFrame.describe()` function generates descriptive statistics.\n",
        "\n",
        "`pandas.DataFrame.head(n=5)` function returns the frist *n* rows."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data.describe()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data.head(5)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training data\n",
        "\n",
        "Let's take a look at the distribuition plot for the distance of the flights in our training dataset. We will be using `seaborn` as our thirdparty library that will help us plot the data.\n",
        "\n",
        "`seaborn.set()` function is used to set *theme* parameters.\n",
        "\n",
        "`seabor.displot()` function provides access to several approaches for visualizing the univariate or bivariate distribution of data, including subsets of data defined by semantic mapping and faceting across multiple subplots.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import seaborn as sns\r\n",
        "\r\n",
        "sns.set(color_codes='True')\r\n",
        "col_list = ['Distance']\r\n",
        "df_col = data[col_list]\r\n",
        "# sns.distplot(df_col)\r\n",
        "sns.displot(df_col)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following pair plot shows the relationship between Distance, CRSDepTime and CRSArrTime.\n",
        "\n",
        "`seaborn.pairplot` plots pairwise relationships in a dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "col_list = ['Distance', 'CRSDepTime', 'CRSArrTime']\r\n",
        "df_2 = data[col_list]\r\n",
        "sns.pairplot(df_2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "\n",
        "The `Pipeline()` function purpose is to assemble several steps that can be cross-validated together while setting different parameters.\n",
        "\n",
        "The `sklearn.linear_model.LogisticRegression` class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers.\n",
        "\n",
        "The `sklearn.preprocessing.StandardScaler()` function standardizes features by removing the mean and scaling to unit variance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scripts.utils import *\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch dataset from the run by name\n",
        "synth_df, Y = prepareDataset(data)\n",
        "\n",
        "#Split dataset\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = split_dataset(synth_df, Y)\n",
        "\n",
        "# Setup scikit-learn pipeline\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "\n",
        "clf = Pipeline(steps=[('classifier', LogisticRegression(solver='liblinear', fit_intercept=True))])\n",
        "\n",
        "model = clf.fit(X_train, Y_train)\n",
        "preds = clf.predict(X_test)\n",
        "analyze_model(clf, X_test, Y_test, preds)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Model\n",
        "\n",
        "Next, register the model obtained from the best run. In order to register the model, the function `register()` should be called. This will take care of registering the model obtained from the best run.\n",
        "\n",
        "The Model.register() function registers a model with the provided workspace.\n",
        "\n",
        "For more information on **Model Class**, please visit: [Microsoft Model Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs('models', exist_ok=True)\r\n",
        "\r\n",
        "joblib.dump(value=model, filename=\"models/flight_delay_weather_.pkl\")\r\n",
        "registered_model = Model.register(model_path=\"models/flight_delay_weather_.pkl\",\r\n",
        "                                  model_name='flight_delay_weather_',\r\n",
        "                                  workspace=ws)\r\n",
        "print(\"Registered \", registered_model.id)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch the model from the registry\n",
        "\n",
        "Retrieve a reference to the model we just uploaded."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import Model\n",
        "from scripts.utils import *\n",
        "\n",
        "synth_df, Y = prepareDataset(data)\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = split_dataset(synth_df, Y)\n",
        "model = Model(ws, 'flight_delay_weather_')\n",
        "model.version"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment\n",
        "\n",
        "## Create managed-endpoints directory\n",
        "\n",
        "Create a new directory to hold the configuration files for deploying a managed endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "\r\n",
        "managed_endpoints = './managed-endpoints'\r\n",
        "\r\n",
        "# Working directory\r\n",
        "if not os.path.exists(managed_endpoints):\r\n",
        "    os.makedirs(managed_endpoints)\r\n",
        "    \r\n",
        "if os.path.exists(os.path.join(managed_endpoints,\".amlignore\")):\r\n",
        "  os.remove(os.path.join(managed_endpoints,\".amlignore\"))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Scoring File\r\n",
        "\r\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome.\r\n",
        "\r\n",
        "The first thing to do in the scoring file is to fetch the model. This is done by calling `Model.get_model_path()` and passing the model name as a parameter.\r\n",
        "\r\n",
        "After the model has been loaded, the function `model.predict()` function should be called to start the scoring process.\r\n",
        "\r\n",
        "For more information on **Machine Learning - Score**, please visit: [Microsoft Machine Learning - Score Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-score)\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/score.py\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azureml.automl.core\n",
        "from sklearn.externals import joblib\n",
        " \n",
        "def init():\n",
        "    global model\n",
        "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
        "    \n",
        "    # this name is model.id of model that we want to deploy\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"flight_delay_weather_.pkl\")\n",
        "    \n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "def run(data):\n",
        "    try:\n",
        "        data = json.loads(data)\n",
        "        df = pd.DataFrame(data['data'], columns=['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime',\n",
        "       'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance',\n",
        "       'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon',\n",
        "       'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp',\n",
        "       'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax',\n",
        "       'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp']) \n",
        "        result = model.predict(df)\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "        print(result)\n",
        "        return {\"error\": result}\n",
        "    return {\"result\":result.tolist()}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the environment definition\r\n",
        "\r\n",
        "The following file contains the details of the environment to host the model and code. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/score-new.yml\n",
        "name: fd-local-mng-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - numpy\n",
        "  - pip\n",
        "  - scikit-learn==0.22.1\n",
        "  - scipy\n",
        "  - pip:\n",
        "    - azureml-defaults\n",
        "    - azureml-sdk[notebooks,automl]\n",
        "    - pandas\n",
        "    - inference-schema[numpy-support]\n",
        "    - joblib\n",
        "    - numpy\n",
        "    - scipy"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the endpoint configuration\r\n",
        "Specific inputs are required to deploy a model on an online endpoint:\r\n",
        "\r\n",
        "1. Model files.\r\n",
        "1. The code that's required to score the model.\r\n",
        "1. An environment in which your model runs.\r\n",
        "1. Settings to specify the instance type and scaling capacity."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/endpointconfig.yml\r\n",
        "name: fd-local-mng-endpoint\r\n",
        "type: online\r\n",
        "auth_mode: key\r\n",
        "traffic:\r\n",
        "  blue: 100\r\n",
        "\r\n",
        "deployments:\r\n",
        "  #blue deployment\r\n",
        "  - name: blue\r\n",
        "    model: azureml:flight_delay_weather_:1\r\n",
        "    code_configuration:\r\n",
        "      code:\r\n",
        "        local_path: ./\r\n",
        "      scoring_script: score.py\r\n",
        "    environment: \r\n",
        "      name: fd-local-mng-env\r\n",
        "      version: 1\r\n",
        "      path: ./\r\n",
        "      conda_file: file:./score-new.yml\r\n",
        "      docker:\r\n",
        "          image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\r\n",
        "    instance_type: Standard_DS3_v2\r\n",
        "    scale_settings:\r\n",
        "      scale_type: manual\r\n",
        "      instance_count: 1\r\n",
        "      min_instances: 1\r\n",
        "      max_instances: 2"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy your managed online endpoint to Azure\r\n",
        "\r\n",
        "This deployment might take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment will finish processing more quickly."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint create -g [your resource group name] -w [your AML workspace name] -n fd-local-mng-endpoint -f ./managed-endpoints/endpointconfig.yml"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a sample request JSON file\n",
        "\n",
        "Export some test data to a JSON file we can send to the endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/sample-request.json\r\n",
        "{\"data\": [\r\n",
        "[6.0,21.0,6.0,1330.0,1600.0,9.0,150.0,16.0,93.0,745.0,33.64044444,-84.42694444,8.0,40.69249722,-74.16866056,29.0,51148.8,53568.0,2.0,0.0,438.4,451.2,0.0,0.0,30.5,28.5,18.0,15.0,2040.0,1720.0],\r\n",
        "[4.0,2.0,3.0,1910.0,2035.0,11.0,85.0,222.0,62.0,361.0,35.87763889,-78.78747222,25.0,39.99798528,-82.89188278,33.0,44928.0,45273.6,0.0,0.0,355.2,438.4,0.0,0.0,23.0,12.5,12.0,1.5,1400.0,680.0],\r\n",
        "[1.0,3.0,4.0,935.0,1224.0,16.0,229.0,207.0,78.0,1302.0,39.87195278,-75.24114083,36.0,32.89595056,-97.0372,41.0,33177.6,35596.8,0.0,0.0,156.8,252.8,0.0,0.0,-2.0,6.5,-8.0,-4.5,320.0,440.0],\r\n",
        "[4.0,3.0,4.0,1000.0,1252.0,16.0,172.0,207.0,206.0,951.0,39.87195278,-75.24114083,36.0,26.68316194,-80.09559417,7.0,45273.6,44582.4,0.0,4.0,425.6,220.8,0.0,0.0,12.0,28.0,0.5,22.5,640.0,2720.0],\r\n",
        "[1.0,21.0,1.0,800.0,1045.0,15.0,105.0,198.0,129.0,589.0,41.979595,-87.90446417,12.0,38.94453194,-77.45580972,43.0,33868.8,34905.6,2.0,0.0,256.0,246.4,56.0,0.0,-7.0,-3.0,-17.0,-13.5,160.0,200.0],\r\n",
        "[3.0,12.0,3.0,1640.0,1952.0,5.0,192.0,89.0,101.0,1065.0,40.69249722,-74.16866056,29.0,26.07258333,-80.15275,7.0,41817.6,42508.8,0.0,0.0,336.0,368.0,0.0,0.0,10.0,27.0,0.5,19.5,640.0,2280.0],\r\n",
        "[3.0,19.0,3.0,1229.0,1346.0,6.0,77.0,151.0,76.0,214.0,40.77724306,-73.87260917,32.0,38.85208333,-77.03772222,43.0,42854.4,42854.4,22.0,0.0,204.8,307.2,0.0,0.0,10.0,15.0,4.0,6.5,800.0,960.0],\r\n",
        "[4.0,18.0,5.0,1210.0,1503.0,4.0,173.0,139.0,169.0,944.0,40.63975111,-73.77892556,32.0,28.42888889,-81.31602778,7.0,47692.8,45964.8,0.0,0.0,524.8,508.8,0.0,0.0,22.5,26.0,5.5,11.5,920.0,1360.0],\r\n",
        "[11.0,1.0,6.0,615.0,745.0,9.0,90.0,130.0,18.0,432.0,39.71732917,-86.29438417,13.0,33.64044444,-84.42694444,8.0,36633.6,38016.0,0.0,0.0,297.6,387.2,0.0,0.0,22.0,20.5,5.0,2.0,880.0,720.0],\r\n",
        "[11.0,24.0,1.0,936.0,1123.0,8.0,107.0,208.0,77.0,602.0,33.43416667,-112.00805559999999,3.0,39.85840806,-104.6670019,5.0,35942.4,34214.4,0.0,0.0,297.6,291.2,0.0,0.0,27.5,17.0,10.0,-9.5,520.0,280.0]]}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoke the endpoint to score data by using your model\r\n",
        "\r\n",
        "You can use either the invoke command or a REST client of your choice to invoke the endpoint and score against it.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint invoke -g [your resource group name] -w [your AML workspace name] -n fd-local-mng-endpoint --request-file ./managed-endpoints/sample-request.json"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Create/connect to the Kubernetes compute cluster \r\n",
        "\r\n",
        "The `AksCompute Class` manages an Azure Kubernetes Service compute target in Azure Machine Learning.\r\n",
        "\r\n",
        "The `ComputeTarget Class` is an abstract parent class for all compute targets managed by Azure Machine Learning. A compute target is a designated compute resource/environment where you run your training script or host your service deployment. \r\n",
        "\r\n",
        "The `ComputeTargetException` is an exception related to failures when creating, interacting with, or configuring a compute target. This exception is commonly raised for failures attaching a compute target, missing headers, and unsupported configuration values.\r\n",
        "\r\n",
        "For more information on **AksCompute Class**, please visit: [Microsoft Machine Learning - AksCompute Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.aks.akscompute?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **ComputeTarget Class**, please visit: [Microsoft Machine Learning - ComputeTarget Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py)\r\n",
        "\r\n",
        "For more information on **ComputeTargetException Class**, please visit: [Microsoft Machine Learning - ComputeTargetException Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.exceptions.computetargetexception?view=azure-ml-py)\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.compute import AksCompute\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "from azureml.exceptions import ComputeTargetException\r\n",
        "\r\n",
        "prov_config = AksCompute.provisioning_configuration(location='westus2')\r\n",
        "\r\n",
        "try:\r\n",
        "    aks_target = AksCompute(ws, 'flight-delay-aks')\r\n",
        "except ComputeTargetException:\r\n",
        "    # Create the cluster\r\n",
        "    aks_target = ComputeTarget.create(workspace = ws, \r\n",
        "                            name = 'flight-delay-aks', \r\n",
        "                            provisioning_configuration = prov_config)\r\n",
        "    aks_target.wait_for_completion(True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Create Scoring File\r\n",
        "\r\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome.\r\n",
        "\r\n",
        "The first thing to do in the scoring file is to fetch the model. This is done by calling `Model.get_model_path()` and passing the model name as a parameter.\r\n",
        "\r\n",
        "After the model has been loaded, the function `model.predict()` function should be called to start the scoring process.\r\n",
        "\r\n",
        "For more information on **Machine Learning - Score**, please visit: [Microsoft Machine Learning - Score Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-score)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile score.py\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        " \n",
        "def init():\n",
        "    global model\n",
        "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
        "    \n",
        "    # this name is model.id of model that we want to deploy\n",
        "    model_path = Model.get_model_path(model_name = 'flight_delay_weather_')\n",
        "    \n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "def run(data):\n",
        "    try:\n",
        "        data = json.loads(data)\n",
        "        df = pd.DataFrame(data['data'], columns=['Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime',\n",
        "       'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance',\n",
        "       'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon',\n",
        "       'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp',\n",
        "       'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax',\n",
        "       'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp']) \n",
        "        result = model.predict(df)\n",
        "    except Exception as e:\n",
        "        result = str(e)\n",
        "        print(result)\n",
        "        return {\"error\": result}\n",
        "    return {\"result\":result.tolist()}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Deploy the model to Kubernetes\n",
        "\n",
        "Now that the AKS cluster has been deployed , it’s time to create an `InferenceConfig` object by calling its constructor and passing the runtime type, the path to the `entry_script` (score.py), and the `conda_file` (the file that holds the environment dependencies).\n",
        "\n",
        "Next, define the configuration of the web service to deploy. This is done by calling `AksWebservice.deploy_configuration()` and passing along the number of `cpu_cores` and `memory_gb` that the service needs.\n",
        "\n",
        "Finally, in order to deploy the model and service to the created AKS cluster, the function `Model.deploy()` should be called, passing along the workspace object, a list of models to deploy, the defined inference configuration, deployment configuration, and the AKS object created in the step above.\n",
        "\n",
        "For more information on **InferenceConfig**, please visit: [Microsoft InferenceConfig Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py)\n",
        "\n",
        "For more information on **AksWebService**, please visit: [Microsoft AksWebService Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py)\n",
        "\n",
        "For more information on **Model**, please visit: [Microsoft Model Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)\n",
        "\n",
        "\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import InferenceConfig\r\n",
        "from azureml.core.webservice import AksWebservice\r\n",
        "from azureml.exceptions import WebserviceException\r\n",
        "\r\n",
        "inference_config = InferenceConfig(runtime= \"python\",\r\n",
        "                                    entry_script=\"score.py\",\r\n",
        "                                    conda_file=\"score-new.yml\")\r\n",
        "\r\n",
        "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, \r\n",
        "                                                        memory_gb = 1)\r\n",
        "\r\n",
        "try:\r\n",
        "    service = AksWebservice(ws, 'flight-delay-local')\r\n",
        "    print(service.state)\r\n",
        "except WebserviceException:\r\n",
        "    service = model.deploy(ws, \r\n",
        "                            'flight-delay-local', \r\n",
        "                            [model], \r\n",
        "                            inference_config, \r\n",
        "                            deployment_config, \r\n",
        "                            aks_target\r\n",
        "                            )\r\n",
        "\r\n",
        "    service.wait_for_deployment(show_output = True)\r\n",
        "    print(service.state)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Connect to the deployed webservice\n",
        "\n",
        "Now with test data, we can get it into a suitable format to consume the web service. First an instance of the web service should be obtained by calling the constructor `Webservice()` with the Workspace object and the service name as parameters. Sanitizing of the data is then performed in order to avoid sending unexpected columns to the web service. Finally, call the service via POST using the `requests` module. `requests.post()` will call the deployed web service. It takes for parameters the service URL, the test data, and a headers dictionary that contains the authentication token.\n",
        "\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "import csv\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "from azureml.core.webservice import Webservice\r\n",
        "\r\n",
        "aks_service = Webservice(ws, 'flight-delay-local')\r\n",
        "aks_service.update(enable_app_insights=True)\r\n",
        "\r\n",
        "# prepare the test data\r\n",
        "sample = X_test.sample(n=10, random_state=4).values.tolist()\r\n",
        "\r\n",
        "headers = {'Content-Type':'application/json'}\r\n",
        "\r\n",
        "if aks_service.auth_enabled:\r\n",
        "    headers['Authorization'] = 'Bearer '+ aks_service.get_keys()[0]\r\n",
        "\r\n",
        "output_df = []\r\n",
        "for x in sample:\r\n",
        "    test_sample = json.dumps({'data': [x]})\r\n",
        "    response = requests.post(aks_service.scoring_uri, data=test_sample, headers=headers)\r\n",
        "    prediction = [response.json()['result'][0]]\r\n",
        "    prediction.extend(x)\r\n",
        "    output_df.append(prediction)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Present scoring service predictions\n",
        "\n",
        "Let's format our service responses and present them in a suitable way to our end users."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def highlight_delays(val):\r\n",
        "    return 'background-color: yellow' if val == True else ''\r\n",
        "\r\n",
        "predictions = pd.DataFrame(output_df, columns =['Prediction', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance', 'Origin_Lat', 'Origin_Lon', 'Origin_State', 'Dest_Lat', 'Dest_Lon', 'Dest_State', 'Origin_dayl', 'Dest_dayl', 'Origin_prcp', 'Dest_prcp', 'Origin_srad', 'Dest_srad', 'Origin_swe', 'Dest_swe', 'Origin_tmax', 'Dest_tmax', 'Origin_tmin', 'Dest_tmin', 'Origin_vp', 'Dest_vp'])\r\n",
        "predictions = predictions.style.applymap(highlight_delays, subset=['Prediction'])\r\n",
        "predictions"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}