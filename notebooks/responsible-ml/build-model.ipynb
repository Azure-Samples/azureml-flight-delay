{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Responsible ML - Fairness, Explainability, and Error Analysis\n",
        "\n",
        "## Install prerequisites\n",
        "\n",
        "Before running the notebook, make sure the correct versions of these libraries are installed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install azureml-sdk --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install fairlearn==0.6.2 --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install azureml-contrib-interpret --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install azureml-contrib-fairness --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install azureml-interpret --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install gevent requests flask flask-cors"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install interpret-community --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install --upgrade azureml.contrib.interpret"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install raiwidgets==0.8.0 --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install azureml-interpret --upgrade"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ---- Restart Kernel ----"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create working directory\n",
        "\n",
        "The cell below creates our working directory. This will hold our generated scripts."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import warnings\r\n",
        "import os\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "project_folder = './scripts'\r\n",
        "\r\n",
        "if not os.path.exists(project_folder):\r\n",
        "    os.makedirs(project_folder)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973309921
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write utils.py into working directory\n",
        "\n",
        "The `sklearn.preprocessing.LabelEncoder` encodes target labels with value between 0 and n_classes-1.\n",
        "\n",
        "The `sklearn.model_selection.train_test_split` splits arrays or matrices into random train and test subsets\n",
        "\n",
        "The `sklearn.metrics.accuracy_score` is an accuracy classification score. In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
        "\n",
        "The `sklearn.metrics.confusion_matrix` is compute confusion matrix to evaluate the accuracy of a classification.\n",
        "\n",
        "The `sklearn.metrics.f1_score` computes the F1 score, also known as balanced F-score or F-measure. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
        "\n",
        "The `sklearn.metrics.precision_score` computes the precision. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives.\n",
        "\n",
        "The `sklearn.metrics.recall_score` computes the recall. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The `sklearn.metrics.roc_auc_score` computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
        "\n",
        "The `sklearn.metrics.roc_curve` computes Receiver operating characteristic (ROC).\n",
        "\n",
        "The `Model Class` represents the result of machine learning training. A model is the result of a Azure Machine learning training Run or some other model training process outside of Azure. Regardless of how the model is produced, it can be registered in a workspace, where it is represented by a name and a version. \n",
        "\n",
        "\n",
        "For more information on **Model Class**, please visit: [Microsoft Model Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $project_folder/utils.py\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from azureml.core import Model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "from azureml.core import Model\n",
        "\n",
        "def split_dataset(X_raw, Y):\n",
        "    A = X_raw[['SEX','RACE']]\n",
        "    X = X_raw.drop(labels=['SEX', 'RACE'],axis = 1)\n",
        "    X = pd.get_dummies(X)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(Y)\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_raw, \n",
        "                                                        Y, \n",
        "                                                        A,\n",
        "                                                        test_size = 0.2,\n",
        "                                                        random_state=123,\n",
        "                                                        stratify=Y)\n",
        "\n",
        "    # Work around indexing bug\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    A_train = A_train.reset_index(drop=True)\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    A_test = A_test.reset_index(drop=True)\n",
        "\n",
        "    # Improve labels\n",
        "    A_test.SEX.loc[(A_test['SEX'] == 0)] = 'female'\n",
        "    A_test.SEX.loc[(A_test['SEX'] == 1)] = 'male'\n",
        "\n",
        "    A_test.RACE.loc[(A_test['RACE'] == 0)] = 'Amer-Indian-Eskimo'\n",
        "    A_test.RACE.loc[(A_test['RACE'] == 1)] = 'Asian-Pac-Islander'\n",
        "    A_test.RACE.loc[(A_test['RACE'] == 2)] = 'Black'\n",
        "    A_test.RACE.loc[(A_test['RACE'] == 3)] = 'Other'\n",
        "    A_test.RACE.loc[(A_test['RACE'] == 4)] = 'White'\n",
        "    return X_train, X_test, Y_train, Y_test, A_train, A_test \n",
        "\n",
        "def prepareDataset(X_raw):\n",
        "    df = X_raw.to_pandas_dataframe()\n",
        "    df = df.drop(columns=['HOURS PER WEEK','COUNTRY'])\n",
        "    df = df.sample(n=30000)\n",
        "    df[list(df.columns)] = df[list(df.columns)].astype(int)\n",
        "    Y = df['SHOULD_APPROVE'].values\n",
        "    synth_df = df.drop(columns=['SHOULD_APPROVE'])\n",
        "    return synth_df, Y\n",
        "\n",
        "def fetch_registered_dataset(ws):\n",
        "    datastore = ws.get_default_datastore()\n",
        "    datastore.upload_files(files=['x_raw.csv'], overwrite=True)\n",
        "    datastore_path = [DataPath(datastore, 'x_raw.csv')]\n",
        "    tabular = Dataset.Tabular.from_delimited_files(path=datastore_path)\n",
        "    return tabular\n",
        "    \n",
        "def analyze_model(clf, X_test, Y_test, preds):\n",
        "    accuracy = accuracy_score(Y_test, preds)\n",
        "    print(f'Accuracy', np.float(accuracy))\n",
        "\n",
        "    precision = precision_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'Precision', np.float(precision))\n",
        "\n",
        "    recall = recall_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'Recall', np.float(recall))\n",
        "\n",
        "    f1score = f1_score(Y_test, preds, average=\"macro\")\n",
        "    print(f'F1 Score', np.float(f1score))\n",
        "\n",
        "    class_names = clf.classes_\n",
        "    fig, ax = plt.subplots()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    sns.heatmap(pd.DataFrame(confusion_matrix(Y_test, preds)), annot=True, cmap='YlGnBu', fmt='g')\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    plt.tight_layout()\n",
        "    plt.title('Confusion Matrix', y=1.1)\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    preds_proba = clf.predict_proba(X_test)[::,1]\n",
        "    fpr, tpr, _ = roc_curve(Y_test, preds_proba, pos_label = clf.classes_[1])\n",
        "    auc = roc_auc_score(Y_test, preds_proba)\n",
        "    plt.plot(fpr, tpr, label=\"data 1, auc=\" + str(auc))\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def register_model(name, model, ws):\n",
        "    print(\"Registering \", name)\n",
        "    model_path = \"models/{0}.pkl\".format(name)\n",
        "    if (name == \"loan_approval_grid_model_30\"):\n",
        "        print (model.coef_)\n",
        "    joblib.dump(value=model, filename=model_path)\n",
        "    registered_model = Model.register(model_path=model_path,\n",
        "                                      model_name=name,\n",
        "                                      workspace=ws)\n",
        "    print(\"Registered \", registered_model.id)\n",
        "    return registered_model.id"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Azure ML\n",
        "\n",
        "In the next cell, we create a new Workspace config object using the `<subscription_id>`, `<resource_group_name>`, and `<workspace_name>`. This will fetch the matching Workspace and prompt you for authentication. Please click on the link and input the provided details.\n",
        "\n",
        "For more information on **Workspace**, please visit: [Microsoft Workspace Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)\n",
        "\n",
        "`<subscription_id>` = You can get this ID from the landing page of your Resource Group.\n",
        "\n",
        "`<resource_group_name>` = This is the name of your Resource Group.\n",
        "\n",
        "`<workspace_name>` = This is the name of your Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "try:    \n",
        "    ws = Workspace(\n",
        "        subscription_id = '<subscription_id>', \n",
        "        resource_group = '<resource_group>', \n",
        "        workspace_name = '<workspace_name>')\n",
        "\n",
        "    # Writes workspace config file\n",
        "    ws.write_config()\n",
        "    \n",
        "    print('Library configuration succeeded')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print('Workspace not found')"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973358309
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Privatized Data\n",
        "\n",
        "Let's retrieve our dataset from the default workspace Datastore."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from scripts.utils import *\r\n",
        "\r\n",
        "tabular = fetch_registered_dataset(ws)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973381981
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model\n",
        "\n",
        "<img align=\"left\" src=\"./images/MLOPs-1.gif\"/>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create experiment\n",
        "\n",
        "In our script, there are four distinct sections:\n",
        "\n",
        "1. Feature encoding for the Scikit-learn training.\n",
        "1. Executing the Scikit-learn experiment\n",
        "\n",
        "\n",
        "The `Pipeline()` function purpose is to assemble several steps that can be cross-validated together while setting different parameters.\n",
        "\n",
        "The `sklearn.linear_model.LogisticRegression` class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers.\n",
        "\n",
        "The `sklearn.preprocessing.StandardScaler()` function standardizes features by removing the mean and scaling to unit variance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from azureml.core import Dataset, Run\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scripts.utils import *\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "run = Run.get_context()\n",
        "\n",
        "# Fetch dataset from the run by name\n",
        "synth_df, Y = prepareDataset(tabular)\n",
        "\n",
        "#Split dataset\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = split_dataset(synth_df, Y)\n",
        "\n",
        "# Setup scikit-learn pipeline\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "preprocessor = ColumnTransformer([('onehot', OneHotEncoder(handle_unknown='ignore'), ['SEX', 'RACE',\n",
        "                                                                                    'WORKCLASS', 'MARITAL STATUS', \n",
        "                                                                                      'OCCUPATION','RELATIONSHIP']),\n",
        "                                       ('scaler', StandardScaler(), ['CAPITAL GAIN', 'CAPITAL LOSS', \n",
        "                                                                     'EDUCATION-NUM'])])\n",
        "\n",
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier', LogisticRegression(solver='liblinear', fit_intercept=True))])\n",
        "\n",
        "model = clf.fit(X_train, Y_train)\n",
        "preds = clf.predict(X_test)\n",
        "analyze_model(clf, X_test, Y_test, preds)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973382968
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fairlearn\n",
        "\n",
        "<img align=\"left\" src=\"./images/RespML-FairnessBias.gif\"/>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Artificial intelligence and machine learning systems can display unfair behavior.\n",
        "\n",
        "Let's use Fairlearn open-source Python package with Azure Machine Learning to perform the following tasks:\n",
        "\n",
        "* Assess the fairness of your model predictions. To learn more about fairness in machine learning, see the fairness in machine learning article.\n",
        "* Upload, list and download fairness assessment insights to/from Azure Machine Learning studio.\n",
        "* See a fairness assessment dashboard in Azure Machine Learning studio to interact with your model(s)' fairness insights.\n",
        "\n",
        "The FairlearnDashboard class, wraps the dashboard component."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from fairlearn.widget import FairlearnDashboard\r\n",
        "\r\n",
        "FairlearnDashboard(sensitive_features=A_test,\r\n",
        "                   sensitive_feature_names=['Sex', 'Race'],\r\n",
        "                   y_true=Y_test.tolist(),\r\n",
        "                   y_pred=[preds.tolist()])"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973387077
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InterpretML\n",
        "\n",
        "As our next step we will retrieve our trained model and instantiate the Explainability Dashboard with the data we encoded above.\n",
        "\n",
        "After the Explainability Dashboard has loaded you will be able to navigate through the user interface to identify the most important features of your new model.\n",
        "\n",
        "<img align=\"left\" src=\"./images/RespML-Explainability.gif\"/>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from interpret.ext.blackbox import KernelExplainer\r\n",
        "\r\n",
        "explainer = KernelExplainer(clf.steps[-1][1], \r\n",
        "                             initialization_examples=X_train, \r\n",
        "                             features=X_train.columns, \r\n",
        "                             classes=['Rejected', 'Approved'], \r\n",
        "                             transformations=preprocessor)\r\n",
        "\r\n",
        "global_explanation = explainer.explain_global(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973771464
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from interpret_community.widget import ExplanationDashboard\r\n",
        "ExplanationDashboard._cdn_path = \"newDash2.js\"\r\n",
        "ExplanationDashboard(global_explanation, model, datasetX=X_test, trueY=Y_test)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973787924
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Analysis\n",
        "\n",
        "The Error Analysis widget helps us to get a deep understanding of how failure is distributed for a model. It also aides in debugging ML errors with active data exploration and interpretability techniques."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from raiwidgets import ErrorAnalysisDashboard\r\n",
        "\r\n",
        "ErrorAnalysisDashboard(global_explanation, model, dataset=X_test, true_y=Y_test)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626973813848
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mitigation\n",
        "\n",
        "Let's retrieve our trained model and prepare our data to take the expected shape for the Fairlearn Dashboard. This can be done by encoding the dataset in the same way that we encoded it within the training script. Once the Fairlearn Dashboard has loaded we will be able to interact with the user interface to detect any unfairness against our dataset.\n",
        "\n",
        "## Mitigate training script\n",
        "\n",
        "The script below will retrain a new model and mitigate the unfairness based on the analysis made above."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "from fairlearn.reductions import DemographicParity, GridSearch, ErrorRate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "from scripts.utils import *\n",
        "\n",
        "# Fetch dataset from the run by name\n",
        "synth_df, Y = prepareDataset(tabular)\n",
        "\n",
        "#Split dataset\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = split_dataset(synth_df, Y)\n",
        "\n",
        "first_sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
        "                   constraints=DemographicParity(),\n",
        "                   grid_size=70)\n",
        "\n",
        "first_sweep.fit(X_train, Y_train, sensitive_features=A_train.SEX)\n",
        "\n",
        "predictors = first_sweep.predictors_"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626974388313
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unfair model vs Mitigated model\n",
        "\n",
        "Below we will instantiate the Fairlearn Dashboard in order to compare the unfair model versus the mitigated model and check the accuracy for both."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from fairlearn.widget import FairlearnDashboard\r\n",
        "from fairlearn.reductions import DemographicParity, ErrorRate\r\n",
        "import joblib\r\n",
        "\r\n",
        "errors, disparities = [], []\r\n",
        "for m in predictors:\r\n",
        "    classifier = lambda X: m.predict(X)\r\n",
        "    \r\n",
        "    error = ErrorRate()\r\n",
        "    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train.SEX)\r\n",
        "    disparity = DemographicParity()\r\n",
        "    disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train.SEX)\r\n",
        "    \r\n",
        "    errors.append(error.gamma(classifier)[0])\r\n",
        "    disparities.append(disparity.gamma(classifier).max())\r\n",
        "    \r\n",
        "all_results = pd.DataFrame( {\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\r\n",
        "\r\n",
        "all_models_dict = {\"loan_approval_unmitigated\": model}\r\n",
        "dominant_models_dict = {\"loan_approval_unmitigated\": model}\r\n",
        "base_name_format = \"loan_approval_grid_model_{0}\"\r\n",
        "row_id = 0\r\n",
        "for row in all_results.itertuples():\r\n",
        "    model_name = base_name_format.format(row_id)\r\n",
        "    all_models_dict[model_name] = row.predictor\r\n",
        "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"]<=row.disparity]\r\n",
        "    if row.error <= errors_for_lower_or_eq_disparity.min():\r\n",
        "        dominant_models_dict[model_name] = row.predictor\r\n",
        "    row_id = row_id + 1\r\n",
        "\r\n",
        "dashboard_all = dict()\r\n",
        "models_all = dict()\r\n",
        "for name, predictor in all_models_dict.items():\r\n",
        "    value = predictor.predict(X_test)\r\n",
        "    dashboard_all[name] = value\r\n",
        "    models_all[name] = predictor\r\n",
        "    \r\n",
        "dominant_all = dict()\r\n",
        "for n, p in dominant_models_dict.items():\r\n",
        "    dominant_all[n] = p.predict(X_test)\r\n",
        "    \r\n",
        "FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['Sex', 'Race'],\r\n",
        "                   y_true=Y_test.tolist(), y_pred=dominant_all)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626974394718
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register all models\n",
        "\n",
        "Register all models with Azure ML."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.makedirs('models', exist_ok=True)\r\n",
        "\r\n",
        "model_name_id_mapping = dict()\r\n",
        "for name, model in models_all.items():\r\n",
        "    m_id = register_model(name, model, ws)\r\n",
        "    model_name_id_mapping[name] = m_id\r\n",
        "\r\n",
        "dominant_all_ids = dict()\r\n",
        "for name, y_pred in dominant_all.items():\r\n",
        "    dominant_all_ids[model_name_id_mapping[name]] = y_pred"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626974691609
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register model :30\n",
        "\n",
        "Register our preferred model with Azure ML."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core import Model\r\n",
        "\r\n",
        "model_name=\"loan_approval_grid_model_30\"\r\n",
        "model=models_all[model_name]\r\n",
        "print(\"Registering \", model_name)\r\n",
        "model_path = \"models/{0}.pkl\".format(model_name)\r\n",
        "print (model.coef_)\r\n",
        "joblib.dump(value=model, filename=model_path)\r\n",
        "registered_model = Model.register(model_path=model_path,\r\n",
        "                                  model_name=model_name,\r\n",
        "                                  workspace=ws)\r\n",
        "print(\"Registered \", registered_model.id)"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626974750654
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading a dashboard\n",
        "\n",
        "We create a _dashboard dictionary_ using Fairlearn's `metrics` package. The `_create_group_metric_set` method has arguments similar to the Dashboard constructor, except that the sensitive features are passed as a dictionary (to ensure that names are available), and we must specify the type of prediction. Note that we use the `dashboard_registered` dictionary we just created:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\r\n",
        "from fairlearn.metrics._group_metric_set import _create_group_metric_set\r\n",
        "\r\n",
        "sf = { 'sex': A_test.SEX, 'race': A_test.RACE }\r\n",
        "dash_dict_all = _create_group_metric_set(y_true=Y_test,\r\n",
        "                                         predictions=dominant_all_ids,\r\n",
        "                                         sensitive_features=sf,\r\n",
        "                                         prediction_type='binary_classification')"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626974765592
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Explanations\n",
        "\n",
        "The Experiment constructor allows to create an experiment instance. The constructor takes in the current workspace, and an experiment name. \n",
        "\n",
        "The ExplanationClient object defines the client that uploads and downloads explanations. \n",
        "\n",
        "For more information on **ExplanationClient**, please visit: [Microsoft ExplanationClient Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret.explanationclient?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.interpret import ExplanationClient\r\n",
        "from azureml.core import Experiment\r\n",
        "\r\n",
        "exp = Experiment(ws, \"Loan_Approval_Exp\")\r\n",
        "print(exp)\r\n",
        "\r\n",
        "run = exp.start_logging()\r\n",
        "try:\r\n",
        "    dashboard_title = \"Upload MultiAsset from Grid Search with Loan Approval dataset\"\r\n",
        "    upload_id = upload_dashboard_dictionary(run,\r\n",
        "                                            dash_dict_all,\r\n",
        "                                            dashboard_name=dashboard_title)\r\n",
        "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\r\n",
        "\r\n",
        "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\r\n",
        "    \r\n",
        "    \r\n",
        "finally:\r\n",
        "    run.complete()\r\n",
        "\r\n",
        "client = ExplanationClient.from_run(run)\r\n",
        "client.upload_model_explanation(global_explanation, comment = \"Loan Approval data global explanation\")"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1626987141741
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}