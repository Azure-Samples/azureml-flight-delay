{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Responsible ML - Homomorphic Encryption\n",
        "\n",
        "## Install prerequisites\n",
        "\n",
        "Before running the notebook, make sure the correct versions of these libraries are installed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install encrypted-inference --upgrade"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Azure ML\n",
        "\n",
        "In the next cell, we create a new Workspace config object using the `<subscription_id>`, `<resource_group_name>`, and `<workspace_name>`. This will fetch the matching Workspace and prompt you for authentication. Please click on the link and input the provided details.\n",
        "\n",
        "For more information on **Workspace**, please visit: [Microsoft Workspace Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)\n",
        "\n",
        "`<subscription_id>` = You can get this ID from the landing page of your Resource Group.\n",
        "\n",
        "`<resource_group_name>` = This is the name of your Resource Group.\n",
        "\n",
        "`<workspace_name>` = This is the name of your Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.workspace import Workspace\r\n",
        "import warnings\r\n",
        "\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "try:    \r\n",
        "    ws = Workspace(\r\n",
        "        subscription_id = '<subscription_id>', \r\n",
        "        resource_group = '<resource_group>', \r\n",
        "        workspace_name = '<workspace_name>')\r\n",
        "\r\n",
        "    # Writes workspace config file\r\n",
        "    ws.write_config()\r\n",
        "    \r\n",
        "    print('Library configuration succeeded')\r\n",
        "except Exception as e:\r\n",
        "    print(e)\r\n",
        "    print('Workspace not found')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homomorphic Encryption\n",
        "\n",
        "Homomorphic Encryption refers to a new type of encryption technology that allows computation to be directly on encrypted data, without requiring any decryption in the process. \n",
        "\n",
        "<img src=\"./images/encrypted.png\" alt=\"Forest\" style=\"display: inline-block;margin-left: auto;margin-right: auto;width:45%\">"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Model from registry\n",
        "\n",
        "Next, fetch the latest model from our model registry."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import Model\n",
        "from scripts.utils import *\n",
        "\n",
        "tabular = fetch_registered_dataset(ws)\n",
        "synth_df, Y = prepareDataset(tabular)\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test = split_dataset(synth_df, Y)\n",
        "model = Model(ws, 'loan_approval_grid_model_30')\n",
        "model.version"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create managed-endpoints directory\n",
        "\n",
        "Create a new directory to hold the configuration files for deploying a managed endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\r\n",
        "\r\n",
        "managed_endpoints = './managed-endpoints'\r\n",
        "\r\n",
        "# Working directory\r\n",
        "if not os.path.exists(managed_endpoints):\r\n",
        "    os.makedirs(managed_endpoints)\r\n",
        "    \r\n",
        "if os.path.exists(os.path.join(managed_endpoints,\".amlignore\")):\r\n",
        "  os.remove(os.path.join(managed_endpoints,\".amlignore\"))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Scoring File\r\n",
        "\r\n",
        "Creating the scoring file is next step before deploying the service. This file is responsible for the actual generation of predictions using the model. The values or scores generated can represent predictions of future values, but they might also represent a likely category or outcome.\r\n",
        "\r\n",
        "The first thing to do in the scoring file is to fetch the model. This is done by calling `Model.get_model_path()` and passing the model name as a parameter.\r\n",
        "\r\n",
        "After the model has been loaded, the function `model.predict()` function should be called to start the scoring process.\r\n",
        "\r\n",
        "For more information on **Machine Learning - Score**, please visit: [Microsoft Machine Learning - Score Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-score)\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/score.py\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from azureml.core.model import Model\n",
        "import joblib\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from encrypted.inference.eiserver import EIServer\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    # this name is model.id of model that we want to deploy\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"loan_approval_grid_model_30.pkl\")\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    global server\n",
        "    server = EIServer(model.coef_, model.intercept_, verbose=True)\n",
        "\n",
        "def run(raw_data):\n",
        "    json_properties = json.loads(raw_data)\n",
        "\n",
        "    key_id = json_properties['key_id']\n",
        "    conn_str = json_properties['conn_str']\n",
        "    container = json_properties['container']\n",
        "    data = json_properties['data']\n",
        "\n",
        "    # download the Galois keys from blob storage \n",
        "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container, blob=key_id)\n",
        "    public_keys = blob_client.download_blob().readall()\n",
        "    \n",
        "    result = {}\n",
        "    # make prediction\n",
        "    result = server.predict(data, public_keys)\n",
        "\n",
        "    # you can return any data type as long as it is JSON-serializable\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the environment definition\r\n",
        "\r\n",
        "The following file contains the details of the environment to host the model and code. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/score-new.yml\n",
        "name: loan-managed-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.7\n",
        "  - numpy\n",
        "  - pip\n",
        "  - scikit-learn==0.22.1\n",
        "  - scipy\n",
        "  - pip:\n",
        "    - azureml-defaults\n",
        "    - azureml-sdk[notebooks,automl]\n",
        "    - pandas\n",
        "    - inference-schema[numpy-support]\n",
        "    - joblib\n",
        "    - numpy\n",
        "    - scipy\n",
        "    - encrypted-inference==0.9\n",
        "    - azure-storage-blob"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the endpoint configuration\r\n",
        "Specific inputs are required to deploy a model on an online endpoint:\r\n",
        "\r\n",
        "1. Model files.\r\n",
        "1. The code that's required to score the model.\r\n",
        "1. An environment in which your model runs.\r\n",
        "1. Settings to specify the instance type and scaling capacity."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%writefile $managed_endpoints/endpointconfig.yml\n",
        "name: loan-managed-endpoint\n",
        "type: online\n",
        "auth_mode: key\n",
        "traffic:\n",
        "  blue: 100\n",
        "\n",
        "deployments:\n",
        "  #blue deployment\n",
        "  - name: blue\n",
        "    model: azureml:loan_approval_grid_model_30:1\n",
        "    code_configuration:\n",
        "      code:\n",
        "        local_path: ./\n",
        "      scoring_script: score.py\n",
        "    environment: \n",
        "      name: loan-managed-env\n",
        "      version: 1\n",
        "      path: ./\n",
        "      conda_file: file:./score-new.yml\n",
        "      docker:\n",
        "          image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1\n",
        "    instance_type: Standard_DS3_v2\n",
        "    scale_settings:\n",
        "      scale_type: manual\n",
        "      instance_count: 1\n",
        "      min_instances: 1\n",
        "      max_instances: 2"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment\n",
        "\n",
        "<img align=\"center\" src=\"./images/MLOPs-2.gif\"/>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy your managed online endpoint to Azure\r\n",
        "\r\n",
        "This deployment might take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment will finish processing more quickly."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint create -g [your resource group name] -w [your AML workspace name] -n loan-managed-endpoint -f ./managed-endpoints/endpointconfig.yml"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create public and private keys\n",
        "\n",
        "In order to work with Homomorphic Encryption we need to generate our private and public keys to satisfy the encryption process.\n",
        "\n",
        "`EILinearRegressionClient` allows us to create a homomorphic encryption based client, and public keys.\n",
        "\n",
        "To register our training data with our Workspace we need to get the data into the data store. The Workspace will already have a default data store. The function `ws.get_default_datastore()` returns an instance of the data store associated with the Workspace.\n",
        "\n",
        "For more information on **Datastore**, please visit: [Microsoft Datastore Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore?view=azure-ml-py)\n",
        "\n",
        "For more information on **How to deploy an encrypted inferencing web service**, please visit: [Microsoft How to deploy an encrypted inferencing web service Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-homomorphic-encryption-seal)\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore\n",
        "from encrypted.inference.eiclient import EILinearRegressionClient\n",
        "\n",
        "# Create a new Encrypted inference client and a new secret key.\n",
        "edp = EILinearRegressionClient(verbose=True)\n",
        "\n",
        "public_keys_blob, public_keys_data = edp.get_public_keys()\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "container_name = datastore.container_name\n",
        "\n",
        "# Create a local file and write the keys to it\n",
        "public_keys = open(public_keys_blob, \"wb\")\n",
        "public_keys.write(public_keys_data)\n",
        "public_keys.close()\n",
        "\n",
        "# Upload the file to blob store\n",
        "datastore.upload_files([public_keys_blob])\n",
        "\n",
        "# Delete the local file\n",
        "os.remove(public_keys_blob)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sample_index = 4\n",
        "\n",
        "print(X_test.iloc[sample_index].to_frame())\n",
        "inputData = X_test.iloc[sample_index]\n",
        "sample_data = (X_test.to_numpy())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "raw_data = edp.encrypt(sample_data[sample_index])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Service with Encrypted data\n",
        "\n",
        "Now with test data, we can get it into a suitable format to consume the web service. First an instance of the web service should be obtained by calling the constructor `Webservice()` with the Workspace object and the service name as parameters. \n",
        "\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "\n",
        "#pass the connection string for blob storage to give the server access to the uploaded public keys \n",
        "conn_str_template = 'DefaultEndpointsProtocol={};AccountName={};AccountKey={};EndpointSuffix=core.windows.net'\n",
        "conn_str = conn_str_template.format(datastore.protocol, datastore.account_name, datastore.account_key)\n",
        "\n",
        "#build the json \n",
        "data = json.dumps({\"data\": raw_data, \"key_id\" : public_keys_blob, \"conn_str\" : conn_str, \"container\" : container_name })"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a sample request JSON file\n",
        "\n",
        "Export some test data to a JSON file we can send to the endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open(os.path.join(managed_endpoints, 'sample-request.json'), 'w') as file:\n",
        "  file.write(data)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoke the endpoint to score data by using your model\r\n",
        "\r\n",
        "You can use either the invoke command or a REST client of your choice to invoke the endpoint and score against it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!az ml endpoint invoke -g [your resource group name] -w [your AML workspace name] -n loan-managed-endpoint --request-file ./managed-endpoints/sample-request.json > ./managed_endpoints/sample-response.json"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decrypting Service Response\n",
        "\n",
        "The below cell uses the `decrypt()` function to decrypt the response from the deployed service. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "eresult = None\n",
        "with open(os.path.join(managed_endpoints, 'sample-response.json'), 'r') as file:\n",
        "    eresult = json.loads(json.loads(file.read()))\n",
        "\n",
        "results = edp.decrypt(eresult)\n",
        "\n",
        "print ('Decrypted the results ', results)\n",
        "\n",
        "#Apply argmax to identify the prediction result\n",
        "prediction = 'Deny'\n",
        "if results[0] > 0:\n",
        "    prediction = 'Approve'\n",
        "\n",
        "actual = 'Deny'\n",
        "if Y_test[sample_index] == 1:\n",
        "    actual = 'Approve'\n",
        "\n",
        "print ( ' Prediction : ', prediction)\n",
        "print( 'Actual : ', actual)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Deploy to Azure Container Instance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!cp $managed_endpoints/score.py ./score.py"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment dependencies\n",
        "\n",
        "The first step is to define the dependencies that are needed for the service to run and they are defined by calling `CondaDependencies.create()`. This create function will receive as parameters the pip and conda packages to install on the remote machine. Secondly, the output of this function is persisted into a `.yml` file that will be leveraged later on the process.\n",
        "\n",
        "Now it's time to create a `InferenceConfig` object by calling its constructor and passing the runtime type, the path to the `entry_script` (score.py), and the `conda_file` (the previously created file that holds the environment dependencies).\n",
        "\n",
        "The `CondaDependencies.create()` function initializes a new CondaDependencies object.\n",
        "\n",
        "For more information on **CondaDependencies**, please visit: [Microsoft CondaDependencies Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)\n",
        "\n",
        "For more information on **InferenceConfig**, please visit: [Microsoft InferenceConfig Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import InferenceConfig, Model\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "azureml_pip_packages = ['azureml-defaults', 'azureml-contrib-interpret', 'azureml-core', 'azureml-telemetry',\n",
        "                        'azureml-interpret', 'azureml-dataprep','azureml-dataprep[fuse,pandas]','joblib',\n",
        "                        'matplotlib','scikit-learn==0.22.1','seaborn','fairlearn','encrypted-inference==0.9','azure-storage-blob']\n",
        "\n",
        "# Define dependencies needed in the remote environment\n",
        "myenv = CondaDependencies.create(pip_packages=azureml_pip_packages)\n",
        "\n",
        "# Write dependencies to yml file\n",
        "with open(\"myenv.yml\",\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())\n",
        "\n",
        "# Create an inference config object based on the score.py and myenv.yml from previous steps\n",
        "inference_config = InferenceConfig(runtime= \"python\",\n",
        "                                    entry_script=\"score.py\",\n",
        "                                    conda_file=\"myenv.yml\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy model to Azure Container Instance\n",
        "\n",
        "In order to deploy the to an Azure Container Instance, the function `Model.deploy()` should be called, passing along the workspace object, service name and list of models to deploy.\n",
        "\n",
        "`Webservice` defines base functionality for deploying models as web service endpoints in Azure Machine Learning. Webservice constructor is used to retrieve a cloud representation of a Webservice object associated with the provided Workspace.\n",
        "\n",
        "The `AciWebService` represents a machine learning model deployed as a web service endpoint on Azure Container Instances. A deployed service is created from a model, script, and associated files. The resulting web service is a load-balanced, HTTP endpoint with a REST API. You can send data to this API and receive the prediction returned by the model.\n",
        "\n",
        "\n",
        "For more information on **Model**, please visit: [Microsoft Model Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)\n",
        "\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Class Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py)\n",
        "\n",
        "For more information on **AciWebservice**, please visit: [Microsoft AciWebservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.aci.aciwebservice?view=azure-ml-py)\n",
        "\n",
        "**Note:** Please wait for the execution of the cell to finish before moving forward."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.exceptions import WebserviceException\n",
        "from azureml.core.model import Model\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                               memory_gb = 2,\n",
        "                                               description = \"Loan approval service\")\n",
        "\n",
        "service_name_aci = 'loan-approval-aci'\n",
        "print(service_name_aci)\n",
        "\n",
        "try:\n",
        "    aci_service = Webservice(ws, service_name_aci)\n",
        "    print(aci_service.state)\n",
        "except WebserviceException:\n",
        "    aci_service = Model.deploy(ws, service_name_aci, [model], inference_config, aciconfig)\n",
        "    aci_service.wait_for_deployment(True)\n",
        "    print(aci_service.state)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Service with Encrypted data\n",
        "\n",
        "Now with test data, we can get it into a suitable format to consume the web service. First an instance of the web service should be obtained by calling the constructor `Webservice()` with the Workspace object and the service name as parameters. \n",
        "\n",
        "For more information on **Webservice**, please visit: [Microsoft Webservice Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice?view=azure-ml-py)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\r\n",
        "from azureml.core import Webservice\r\n",
        "\r\n",
        "service = Webservice(ws, service_name_aci)\r\n",
        "\r\n",
        "#pass the connection string for blob storage to give the server access to the uploaded public keys \r\n",
        "conn_str_template = 'DefaultEndpointsProtocol={};AccountName={};AccountKey={};EndpointSuffix=core.windows.net'\r\n",
        "conn_str = conn_str_template.format(datastore.protocol, datastore.account_name, datastore.account_key)\r\n",
        "\r\n",
        "#build the json \r\n",
        "data = json.dumps({\"data\": raw_data, \"key_id\" : public_keys_blob, \"conn_str\" : conn_str, \"container\" : container_name })\r\n",
        "data = bytes(data, encoding='ASCII')\r\n",
        "\r\n",
        "print ('Making an encrypted inference web service call ')\r\n",
        "eresult = service.run(input_data=data)\r\n",
        "\r\n",
        "print ('Received encrypted inference results')\r\n",
        "print (f'Encrypted results: ...', eresult[0][0:100], '...')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decrypting Service Response\n",
        "\n",
        "The below cell uses the `decrypt()` function to decrypt the response from the deployed ACI Service. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "results = edp.decrypt(eresult)\r\n",
        "\r\n",
        "print ('Decrypted the results ', results)\r\n",
        "\r\n",
        "#Apply argmax to identify the prediction result\r\n",
        "prediction = 'Deny'\r\n",
        "if results[0] > 0:\r\n",
        "    prediction = 'Approve'\r\n",
        "\r\n",
        "actual = 'Deny'\r\n",
        "if Y_test[sample_index] == 1:\r\n",
        "    actual = 'Approve'\r\n",
        "\r\n",
        "print ( ' Prediction : ', prediction)\r\n",
        "print( 'Actual : ', actual)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}